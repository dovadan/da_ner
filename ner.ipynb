{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e3da4b-1735-4600-9168-20bd56d6b588",
   "metadata": {},
   "source": [
    "#### 3.1 Supervised DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65e12c-ead8-470e-93e3-d9d9e752232a",
   "metadata": {},
   "source": [
    "##### 3.1.1 In-domain fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e0ae2-a745-428e-b15c-4e8c0c7e294b",
   "metadata": {},
   "source": [
    "Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3449ab-a809-49f3-bf35-9c31167e1150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df08858bafb644d3bce731305b080f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de6a69b0b53424f8ee82fb5792631be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900b527ad09a4945bfcc0e84a44c128c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c82d4a83c544ca1a0efa7af89b9d30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1249a81228db471784c8781e5a418340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64be2004991f4e26ad0647870ec12ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import typing as tp\n",
    "import inspect\n",
    "\n",
    "conll = datasets.load_dataset(\"conll2003\") # DatasetDict\n",
    "wnut = datasets.load_dataset(\"wnut_17\")\n",
    "\n",
    "conll.save_to_disk(\"datasets/conll2003\")\n",
    "wnut.save_to_disk(\"datasets/wnut_17\")\n",
    "\n",
    "CONLL_NER_TAGS = conll['train'].features['ner_tags'].feature.names # всего 9 тегов\n",
    "WNUT_NER_TAGS = wnut['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65a90fc-77e3-4df8-a69d-baf9c2c426b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6b52a1-6304-4e87-b2e5-26461f54f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1bb58e-1b52-4f3a-abb6-7046d9ba5b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONLL_NER_TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ddc48c-ca1d-40b9-8a85-76e60a0aeb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WNUT_NER_TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329e7480-41e6-4f8b-a7ab-5ed7242dfce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {\n",
    "    'O': 'O',\n",
    "    'B-location': 'B-LOC',\n",
    "    'I-location': 'I-LOC',\n",
    "    'B-group': 'B-ORG',\n",
    "    'B-corporation': 'B-ORG',\n",
    "    'B-person': 'B-PER',\n",
    "    'B-creative-work': 'B-MISC',\n",
    "    'B-product': 'B-MISC',\n",
    "    'I-person': 'I-PER',\n",
    "    'I-creative-work': 'I-MISC',\n",
    "    'I-corporation': 'I-ORG',\n",
    "    'I-group': 'I-ORG',\n",
    "    'I-product': 'I-MISC'\n",
    "}\n",
    "\n",
    "labelindexmapping = {WNUT_NER_TAGS.index(k):CONLL_NER_TAGS.index(v) for k, v in label_mapping.items()}\n",
    "\n",
    "import data_prep\n",
    "\n",
    "converted_wnut = wnut.map(lambda x: data_prep.convert_label_sequence(x, labelindexmapping))\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (pipeline, \n",
    "        AutoModelForTokenClassification, AutoTokenizer, \n",
    "        BertForTokenClassification, BertTokenizer)\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW, BertTokenizer, BertForMaskedLM, set_seed\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "set_seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import data_prep\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "label2id_conll= {'O': 0,\n",
    " 'B-PER': 1,\n",
    " 'I-PER': 2,\n",
    " 'B-ORG': 3,\n",
    " 'I-ORG': 4,\n",
    " 'B-LOC': 5,\n",
    " 'I-LOC': 6,\n",
    " 'B-MISC': 7,\n",
    " 'I-MISC': 8\n",
    "}\n",
    "\n",
    "id2label_conll = {v : k for k, v in label2id_conll.items()}\n",
    "\n",
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "# test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, model.config.label2id)\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "\n",
    "assert tokenizer.decode(test_result['input_ids']) == '[CLS] His name is Jerry Abrahamson [SEP]'\n",
    "\n",
    "                                     #CLS     His  name is    Jerry    Abraham   ##son      SEP\n",
    "assert test_result['text_labels'] == ['O'] + [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\",  \"I-PER\"] + [\"O\"]\n",
    "\n",
    "conll = conll.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "wnut = converted_wnut\n",
    "wnut = wnut.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "conll.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "wnut.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cef350f-4a5d-45d1-9bce-ee4ae7e8706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 4, None]\n",
      "[101, 1230, 1271, 1110, 5466, 7752, 2142, 102]\n",
      "['[CLS]', 'His', 'name', 'is', 'Jerry', 'Abraham', '##son', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import data_prep\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "print(test_result.word_ids())\n",
    "print(test_result['input_ids'])\n",
    "print([tokenizer.decode(token) for token in test_result['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afdd5322-dbbb-477b-bb8a-50205e0f2283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1230, 1271, 1110, 5466, 7752, 2142, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'tokens': ['His', 'name', 'is', 'Jerry', 'Abrahamson'], 'ner_tags': [0, 0, 0, 1, 2], 'labels': [0, 0, 0, 0, 1, 2, 2, 0], 'text_labels': ['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec394c-8baf-4ce7-a78a-cf2578f7e78c",
   "metadata": {},
   "source": [
    "Теперь есть готовые датасеты conll и wnut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8d37d-a91b-4dd7-9ed6-a44d1283074e",
   "metadata": {},
   "source": [
    "Создадим даталоадеры для трейн и теста conll и wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "690e880d-eabc-44b2-a449-3ecf2224a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "batch_size = 16\n",
    "wnut_train_dataloader = torch.utils.data.DataLoader(wnut[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "wnut_test_dataloader = torch.utils.data.DataLoader(wnut[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_train_dataloader = torch.utils.data.DataLoader(conll[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_test_dataloader = torch.utils.data.DataLoader(conll[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e5bed9-b6f3-4033-97db-74f7b37262e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "45\n",
      "14\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "batch=next(iter(wnut_test_dataloader))\n",
    "print(len(batch['ner_tags'][7]))\n",
    "print(len(batch['input_ids'][7]))\n",
    "print(len(batch['tokens'][7]))\n",
    "print(len(batch['labels'][7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7cc9c56-5cbc-4436-9ee9-107ae62945c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = {k : [v] for k, v in test_result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c776366-e727-4aeb-ae4e-f6e24bc2ecc9",
   "metadata": {},
   "source": [
    "Измерим метрики на базовой версии BERTа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e6d659-6d16-469b-b306-68a342ea70ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\" # дообучена на датасете CoNLL-2003\n",
    "model_base = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_base.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1bc7559-61c0-43b8-b038-05688a7c45c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1230, 1271, 1110, 5466, 7752, 2142, 102]],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]],\n",
       " 'tokens': [['His', 'name', 'is', 'Jerry', 'Abrahamson']],\n",
       " 'ner_tags': [[0, 0, 0, 1, 2]],\n",
       " 'labels': [[0, 0, 0, 0, 1, 2, 2, 0]],\n",
       " 'text_labels': [['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed14692-28c9-4b81-974b-62d76cd79643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'input_ids': tensor([[ 101, 1230, 1271, 1110, 5466, 7752, 2142,  102]], device='cuda:0'),\n",
       "             'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       "             'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       "             'tokens': [['His', 'name', 'is', 'Jerry', 'Abrahamson']],\n",
       "             'ner_tags': tensor([[0, 0, 0, 1, 2]], device='cuda:0'),\n",
       "             'labels': tensor([[0, 0, 0, 0, 1, 2, 2, 0]], device='cuda:0'),\n",
       "             'text_labels': [['O',\n",
       "               'O',\n",
       "               'O',\n",
       "               'O',\n",
       "               'B-PER',\n",
       "               'I-PER',\n",
       "               'I-PER',\n",
       "               'O']]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тест, что все адекватно предсказывается\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "batched_test_batch = defaultdict(list)\n",
    "\n",
    "for key, value in test_batch.items():\n",
    "    try:\n",
    "        batched_test_batch[key] = torch.tensor(value)\n",
    "    except ValueError:\n",
    "        batched_test_batch[key] = value\n",
    "\n",
    "for key in batched_test_batch:\n",
    "    if isinstance(batched_test_batch[key], torch.Tensor):\n",
    "        batched_test_batch[key] = batched_test_batch[key].to(device)\n",
    "\n",
    "batched_test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f29e5265-c7c5-4d6c-aaf7-f8e025257c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b9021bd-e365-435b-b0ee-a90b59d14cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d337357-9a64-41a6-8ad3-3fecc9d6bd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted_labels': [['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']],\n",
       " 'loss': tensor(3.5653, device='cuda:0'),\n",
       " 'logits': tensor([[[ 6.8225, -0.2862, -1.0980, -0.3375, -1.1505, -1.0953, -1.7006,\n",
       "           -0.8111, -1.3737],\n",
       "          [ 9.3304, -1.1577, -2.1227, -0.1745, -1.8043, -1.7342, -2.2795,\n",
       "           -0.7647, -1.3315],\n",
       "          [ 9.7455, -1.0748, -1.7252, -0.2097, -1.3579, -2.4633, -1.7645,\n",
       "           -1.0774, -1.2969],\n",
       "          [ 9.3520, -1.2463, -1.9811, -0.2193, -1.5422, -2.2553, -1.8455,\n",
       "           -0.9268, -1.1779],\n",
       "          [-0.8416, -1.2910, -2.3588,  8.7858, -0.5230, -1.6059, -2.6656,\n",
       "           -0.3291, -1.6418],\n",
       "          [-0.7083, -1.2668, -1.3766, -1.0028,  8.9504, -1.9978, -0.7555,\n",
       "           -0.9198, -1.0510],\n",
       "          [ 1.7984, -1.4755, -1.4079, -0.3893,  6.5783, -2.9142, -1.2547,\n",
       "           -0.7049, -1.4439],\n",
       "          [ 4.4946, -0.4864, -1.3231,  1.8384,  0.9670, -3.0517, -1.8221,\n",
       "           -0.5035, -1.0015]]], device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import model_utils\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "from model_utils import NamedEntityPredictor\n",
    "\n",
    "ner_test = NamedEntityPredictor(model_base, tokenizer, id2label_conll)\n",
    "ner_test.predict(batched_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f334839f-37bb-49fb-8860-d6cbf68d10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, get_scheduler\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_ # выполняет обрезку градиентов для избежания их взрыва при обучении\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import importlib\n",
    "import data_prep\n",
    "import model_utils\n",
    "importlib.reload(data_prep)\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "from model_utils import train_eval_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74e90e-a5ad-4aa5-ac85-db35033bc11c",
   "metadata": {},
   "source": [
    "Файн-тюн сразу всего BERTа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11b19aa5-5d06-4399-95ba-25a83ac7bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:05<00:00, 38.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:03<00:00, 22.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:23<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.12357293134074256 \t Test: 0.3769584274218406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:23<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.10321705974638462 \t Test: 0.38193389897545177\n",
      "\n",
      "Evaluating fine-tuned model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:05<00:00, 37.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:03<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.84 (+0.09)  0.69 (-0.15)  0.76 (-0.03)  0.30 (-0.15)   \n",
      "MISC          0.20 (-0.38)  0.66 (-0.04)  0.30 (-0.33)  0.07 (-0.10)   \n",
      "ORG           0.68 (-0.03)  0.85 (+0.03)  0.75 (-0.01)  0.27 (+0.11)   \n",
      "PER           0.11 (-0.25)  0.07 (-0.51)  0.08 (-0.37)  0.30 (+0.00)   \n",
      "micro avg     0.46 (-0.12)  0.56 (-0.18)  0.50 (-0.15)  0.17 (-0.07)   \n",
      "macro avg     0.46 (-0.14)  0.57 (-0.17)  0.47 (-0.19)  0.24 (-0.03)   \n",
      "weighted avg  0.50 (-0.11)  0.56 (-0.18)  0.51 (-0.15)  0.24 (-0.02)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.13 (-0.38)  0.18 (-0.30)  \n",
      "MISC          0.11 (-0.11)  0.09 (-0.10)  \n",
      "ORG           0.24 (-0.06)  0.25 (+0.04)  \n",
      "PER           0.05 (-0.25)  0.09 (-0.21)  \n",
      "micro avg     0.12 (-0.19)  0.14 (-0.13)  \n",
      "macro avg     0.13 (-0.20)  0.15 (-0.14)  \n",
      "weighted avg  0.12 (-0.19)  0.14 (-0.14)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.14 (-0.13)\n",
      "pr     |    0.17 (-0.07)\n",
      "rec    |    0.12 (-0.19)\n",
      "acc    |    0.92 (-0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec9921-6645-4626-a721-7b49cc23d84f",
   "metadata": {},
   "source": [
    "Кажется происходит сильное переобучение под train WNUTа."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3c78a-976d-4aad-ba87-32e6a1b76e3a",
   "metadata": {},
   "source": [
    "Если файн-тюнить только классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61698a16-0b9d-480f-8d08-369911367092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:05<00:00, 37.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:03<00:00, 22.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.4141742690278331 \t Test: 0.6115067956256278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.3634748491043216 \t Test: 0.550726252021613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.32342260077315876 \t Test: 0.4996400620834327\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.2925367337857054 \t Test: 0.4584120437502861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.2690735067210287 \t Test: 0.42674131176353974\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.25196356457513824 \t Test: 0.40386411979978465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.24004674139716817 \t Test: 0.38808650256674965\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.23220971399839496 \t Test: 0.37771314860861976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.22781684008282674 \t Test: 0.3719683326320884\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.22641523799775912 \t Test: 0.37011940050640224\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.76 (+0.01)  0.83 (-0.01)  0.79 (+0.00)  0.50 (+0.05)   \n",
      "MISC          0.53 (-0.05)  0.67 (-0.03)  0.59 (-0.04)  0.15 (-0.02)   \n",
      "ORG           0.68 (-0.03)  0.78 (-0.04)  0.73 (-0.03)  0.26 (+0.10)   \n",
      "PER           0.40 (+0.04)  0.59 (+0.01)  0.48 (+0.03)  0.39 (+0.09)   \n",
      "micro avg     0.59 (+0.01)  0.73 (-0.01)  0.65 (+0.00)  0.29 (+0.05)   \n",
      "macro avg     0.59 (-0.01)  0.72 (-0.02)  0.65 (-0.01)  0.32 (+0.05)   \n",
      "weighted avg  0.61 (+0.00)  0.73 (-0.01)  0.66 (+0.00)  0.32 (+0.06)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.43 (-0.08)  0.47 (-0.01)  \n",
      "MISC          0.17 (-0.05)  0.16 (-0.03)  \n",
      "ORG           0.27 (-0.03)  0.26 (+0.05)  \n",
      "PER           0.26 (-0.04)  0.31 (+0.01)  \n",
      "micro avg     0.26 (-0.05)  0.28 (+0.01)  \n",
      "macro avg     0.28 (-0.05)  0.30 (+0.01)  \n",
      "weighted avg  0.26 (-0.05)  0.28 (+0.00)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.28 (+0.01)\n",
      "pr     |    0.29 (+0.05)\n",
      "rec    |    0.26 (-0.05)\n",
      "acc    |    0.93 (+0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ea38c438-50d0-45a1-b9ed-31417c8147f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.4141742690278331,\n",
       "  0.3634748491043216,\n",
       "  0.32342260077315876,\n",
       "  0.2925367337857054,\n",
       "  0.2690735067210287,\n",
       "  0.25196356457513824,\n",
       "  0.24004674139716817,\n",
       "  0.23220971399839496,\n",
       "  0.22781684008282674,\n",
       "  0.22641523799775912],\n",
       " 'test_loss': [0.6115067956256278,\n",
       "  0.550726252021613,\n",
       "  0.4996400620834327,\n",
       "  0.4584120437502861,\n",
       "  0.42674131176353974,\n",
       "  0.40386411979978465,\n",
       "  0.38808650256674965,\n",
       "  0.37771314860861976,\n",
       "  0.3719683326320884,\n",
       "  0.37011940050640224],\n",
       " 'f1': [0.2751986616478461,\n",
       "  0.2790294627383016,\n",
       "  0.278391515687141,\n",
       "  0.28208616780045354,\n",
       "  0.286509040333797,\n",
       "  0.2857142857142857,\n",
       "  0.28311812529890007,\n",
       "  0.2825350749879052,\n",
       "  0.2801366520253783,\n",
       "  0.2774792379091353],\n",
       " 'pr': [0.2507621951219512,\n",
       "  0.26200162733930027,\n",
       "  0.2660472972972973,\n",
       "  0.2761989342806394,\n",
       "  0.28664192949907236,\n",
       "  0.2897998093422307,\n",
       "  0.2924901185770751,\n",
       "  0.29554655870445345,\n",
       "  0.2958762886597938,\n",
       "  0.29338842975206614],\n",
       " 'rec': [0.3049119555143652,\n",
       "  0.29842446709916587,\n",
       "  0.2919369786839666,\n",
       "  0.2882298424467099,\n",
       "  0.28637627432808155,\n",
       "  0.28174235403151066,\n",
       "  0.2743280815569972,\n",
       "  0.2706209453197405,\n",
       "  0.2659870250231696,\n",
       "  0.2632066728452271],\n",
       " 'acc': [0.9293198659555387,\n",
       "  0.930547033558314,\n",
       "  0.9307830273280785,\n",
       "  0.9313494123755133,\n",
       "  0.9318921980459716,\n",
       "  0.9321989899466654,\n",
       "  0.9323405862085241,\n",
       "  0.9321989899466654,\n",
       "  0.9321989899466654,\n",
       "  0.9320809930617832]}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_metrics_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f14d4-62fd-4497-adf6-95f94956f3d0",
   "metadata": {},
   "source": [
    "Проверка, что токены паддинга игнорируются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f893ee77-3aca-4618-894e-5cd68731c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n"
     ]
    }
   ],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "print(loss_fct.ignore_index)  # должен быть -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "996bdf1a-3b93-4bb4-835a-755a46a03d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13008ace-0bb4-4a3a-9f78-8a635e999800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 16752, 14669,  1158,  2197,  1155,  1189,  1146,  1111,  4911,\n",
       "          117,  1509, 13193,  1138,  1151,  1189,   106,   106,   108, 17607,\n",
       "        11613,  1183,   102], device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e6883-67d7-4506-b27c-1b76cc3f0760",
   "metadata": {},
   "source": [
    "##### 3.1.2  Resampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2971-6da2-498d-bc85-2ef08a22cbfa",
   "metadata": {},
   "source": [
    "Отберем топ 25% примеров из conll по схожести с wnut с помощью бинарного классификатора, затем сделаем fine-tuning модели на этих топ 25 % примерах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aefb5720-a2a9-4139-bd0d-2318ced796b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "columns = [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\", \"domain_label\"]\n",
    "\n",
    "conll_labeled = conll[\"train\"].map(lambda x: {\"domain_label\": int(0)})\n",
    "columns_to_remove = [col for col in conll_labeled.column_names if col not in columns]\n",
    "conll_labeled = conll_labeled.remove_columns(columns_to_remove)\n",
    "\n",
    "wnut_labeled = wnut[\"train\"].map(lambda x: {\"domain_label\": int(1)})\n",
    "columns_to_remove = [col for col in wnut_labeled.column_names if col not in columns]\n",
    "wnut_labeled = wnut_labeled.remove_columns(columns_to_remove)\n",
    "\n",
    "combined = concatenate_datasets([conll_labeled, wnut_labeled]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec93414-e863-4b92-9fb2-6805ab74d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Extracting embeddings: 100%|██████████████████████████████████████████████████████████| 545/545 [00:34<00:00, 15.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# делим combined на k фолдов, учим k классификаторов на остальных k-1 фолдах, добавляем в combined wnut_score\n",
    "# для каждого примера получаем эмбеддинг предложения с помощью [CLF] токена BERTа\n",
    "# для каждого примера из тестового фолда добавляем wnut_score\n",
    "# затем из combined отбираем топ 25 % примеров по этому скору\n",
    "\n",
    "# сначала получить эмбеддинги всех предложений\n",
    "# затем сделать k классификаторов по типу leave-one-out\n",
    "# классификатор X: [batch_size, emb_len] -> wnut_scores: [batch_size]\n",
    "\n",
    "importlib.reload(model_utils)\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    combined,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_prep.PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels']),\n",
    ")\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "    embeddings = model_utils.get_sentence_embeddings(model, batch)  # [batch_size, emb_size]\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)  # [num_rows, emb_size]\n",
    "\n",
    "combined = combined.add_column(\"cls_embedding\", all_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73daa64f-1670-4a02-a2fb-67500c954ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'domain_label', 'cls_embedding'],\n",
       "    num_rows: 17435\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "119195eb-5819-4c8d-86f0-03a9248b767e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17435, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['cls_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "533747b9-ecbf-4c83-99e6-0a55ba662d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 250.20it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 247.09it/s]\n",
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 251.25it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 254.82it/s]\n",
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 254.08it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 235.78it/s]\n",
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 248.25it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 254.70it/s]\n",
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 250.33it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 232.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# теперь добавим wnut_score в combined для дальнейшего отбора\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "wnut_scores = np.zeros(len(combined))\n",
    "\n",
    "indices = np.arange(len(combined))\n",
    "for train_idxs, test_idxs in kf.split(indices):\n",
    "    train_examples = combined.select(train_idxs)\n",
    "    test_examples = combined.select(test_idxs)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_examples, batch_size=32,\n",
    "        collate_fn=data_prep.PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_examples, batch_size=32,\n",
    "        collate_fn=data_prep.PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "    )\n",
    "\n",
    "    X_train, Y_train = [], []\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=\"Train embeddings\"):\n",
    "        embs = torch.stack(batch[\"cls_embedding\"]).cpu().numpy()\n",
    "        domains = torch.stack(batch['domain_label']).cpu().numpy()\n",
    "        X_train.append(embs)\n",
    "        Y_train.append(domains)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    Y_train = np.concatenate(Y_train)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    X_test = []\n",
    "    for batch in tqdm(test_dataloader, desc=\"Test embeddings\"):\n",
    "        embs = torch.stack(batch[\"cls_embedding\"]).cpu().numpy()\n",
    "        X_test.append(embs)\n",
    "    X_test = np.concatenate(X_test)\n",
    "    probas = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    wnut_scores[test_idxs] = probas\n",
    "\n",
    "combined = combined.add_column(\"wnut_score\", wnut_scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "270e629e-0d7e-4948-96d7-1c7ba129dc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'domain_label', 'cls_embedding', 'wnut_score'],\n",
       "    num_rows: 17435\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ac8b17f-8ba2-465e-955d-0a4bc5071d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбираем топ 25 % объектов из conll по схожести с датасетом с wnut\n",
    "\n",
    "conll_subset = combined.filter(lambda x: x[\"domain_label\"] == 0)\n",
    "\n",
    "conll_sorted = conll_subset.sort(\"wnut_score\", reverse=True) \n",
    "\n",
    "# построить гистограмму с wnut_score\n",
    "\n",
    "top_25_count = int(0.25 * len(conll_sorted))\n",
    "top_25_conll = conll_sorted.select(range(top_25_count))\n",
    "\n",
    "top_25_conll = top_25_conll.remove_columns([\n",
    "    col for col in top_25_conll.column_names if col not in ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37fe00-bad9-42b6-bc15-27b870eeec2a",
   "metadata": {},
   "source": [
    "Теперь есть топ 25 % примеров из CONLL для файн-тюна BERTа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74c067dd-8ae1-4fa6-8b83-1909bae77d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3510\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_25_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b4409ed-fcc5-4f65-94ae-c560cb05fe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'text_labels'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45d087-9e87-44bd-a23c-14636f49dfa0",
   "metadata": {},
   "source": [
    "Размеры датасетов wnut['train'] и top_25_conll почти одинаковы. Посмотрим что будет, если файн-тюнить классификатор на top_25_conll с теми же гиперпараметрами, что и для файн-тюна на wnut['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a48601a-74b6-4228-8295-59d1608df8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:05<00:00, 37.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:03<00:00, 22.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 34.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 1.2603430168195204 \t Test: 0.5994891316434483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.9448994383215904 \t Test: 0.5128830778010097\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.673108392750675 \t Test: 0.4423470243259712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.45281158448620273 \t Test: 0.38964642703901103\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.29694188260896637 \t Test: 0.3548698776666029\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.2057333342730999 \t Test: 0.3360696185702159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.15927771781994537 \t Test: 0.3276628636651569\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.13619176188314502 \t Test: 0.32397360957147164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.1250704107636755 \t Test: 0.3224452372042485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 220/220 [00:06<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.12176169727674939 \t Test: 0.32207260588988845\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.09 (-0.66)  0.08 (-0.76)  0.08 (-0.71)  0.05 (-0.40)   \n",
      "MISC          0.03 (-0.55)  0.10 (-0.60)  0.05 (-0.58)  0.03 (-0.14)   \n",
      "ORG           0.01 (-0.70)  0.02 (-0.80)  0.02 (-0.74)  0.06 (-0.10)   \n",
      "PER           0.02 (-0.34)  0.03 (-0.55)  0.03 (-0.42)  0.03 (-0.27)   \n",
      "micro avg     0.04 (-0.54)  0.05 (-0.69)  0.04 (-0.61)  0.04 (-0.20)   \n",
      "macro avg     0.04 (-0.56)  0.06 (-0.68)  0.04 (-0.62)  0.05 (-0.22)   \n",
      "weighted avg  0.04 (-0.57)  0.05 (-0.69)  0.04 (-0.62)  0.04 (-0.22)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.14 (-0.37)  0.08 (-0.40)  \n",
      "MISC          0.06 (-0.16)  0.04 (-0.15)  \n",
      "ORG           0.05 (-0.25)  0.05 (-0.16)  \n",
      "PER           0.03 (-0.27)  0.03 (-0.27)  \n",
      "micro avg     0.06 (-0.25)  0.05 (-0.22)  \n",
      "macro avg     0.07 (-0.26)  0.05 (-0.24)  \n",
      "weighted avg  0.06 (-0.25)  0.05 (-0.23)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.05 (-0.22)\n",
      "pr     |    0.04 (-0.20)\n",
      "rec    |    0.06 (-0.25)\n",
      "acc    |    0.91 (-0.02)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = top_25_conll\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc26d8f3-0c8f-41cb-b098-8d75524e9253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [1.2603430168195204,\n",
       "  0.9448994383215904,\n",
       "  0.673108392750675,\n",
       "  0.45281158448620273,\n",
       "  0.29694188260896637,\n",
       "  0.2057333342730999,\n",
       "  0.15927771781994537,\n",
       "  0.13619176188314502,\n",
       "  0.1250704107636755,\n",
       "  0.12176169727674939],\n",
       " 'test_loss': [0.5994891316434483,\n",
       "  0.5128830778010097,\n",
       "  0.4423470243259712,\n",
       "  0.38964642703901103,\n",
       "  0.3548698776666029,\n",
       "  0.3360696185702159,\n",
       "  0.3276628636651569,\n",
       "  0.32397360957147164,\n",
       "  0.3224452372042485,\n",
       "  0.32207260588988845],\n",
       " 'f1': [0.27060270602706027,\n",
       "  0.26825266611977033,\n",
       "  0.2708247845711941,\n",
       "  0.26813365933170336,\n",
       "  0.21632024634334107,\n",
       "  0.1366691560866318,\n",
       "  0.0848529959062151,\n",
       "  0.058245083207261725,\n",
       "  0.05151864667435602,\n",
       "  0.04833141542002302],\n",
       " 'pr': [0.2426470588235294,\n",
       "  0.24061810154525387,\n",
       "  0.24300441826215022,\n",
       "  0.23927272727272728,\n",
       "  0.18499012508229098,\n",
       "  0.11444652908067542,\n",
       "  0.0708955223880597,\n",
       "  0.049201277955271565,\n",
       "  0.04402102496714849,\n",
       "  0.0412303664921466],\n",
       " 'rec': [0.30583873957367935,\n",
       "  0.3030583873957368,\n",
       "  0.30583873957367935,\n",
       "  0.3049119555143652,\n",
       "  0.26042632066728455,\n",
       "  0.1696014828544949,\n",
       "  0.1056533827618165,\n",
       "  0.07136237256719184,\n",
       "  0.06209453197405004,\n",
       "  0.05838739573679333],\n",
       " 'acc': [0.9282106952376458,\n",
       "  0.9287770802850804,\n",
       "  0.928965875300892,\n",
       "  0.928965875300892,\n",
       "  0.9274791145513759,\n",
       "  0.921532071553311,\n",
       "  0.9162458111105867,\n",
       "  0.9129890970878369,\n",
       "  0.9113371406994856,\n",
       "  0.910888752536933]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_metrics_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5945a-3277-4a49-86d7-e1e608033415",
   "metadata": {},
   "source": [
    "Видно, что максимальное улучшение метрик хуже, чем просто для файн-тюна с теми же гиперпараметрами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765622d-8567-414d-b803-e557c8f19e2c",
   "metadata": {},
   "source": [
    "#### 3.2 Unsupervised DA\n",
    "\n",
    "##### 3.2.1 Proxy-labels methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c82b3-2ca9-422a-a661-8e2acde61985",
   "metadata": {},
   "source": [
    "До этого использовались размеченные данные. Теперь будем отбирать наиболее уверенные с точки зрения пре-тренированной модели примеры, делать предсказания и дообучать на этих прокси-лейблах, чтобы перенести знания из целевого домена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cae3690a-81f1-43af-8e08-f71084c827ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# лучше сделать рестарт ядра перед запуском ячеек ниже\n",
    "# ниже повторная предобработка данных\n",
    "import datasets\n",
    "import typing as tp\n",
    "import inspect\n",
    "\n",
    "conll = datasets.load_dataset(\"conll2003\") # DatasetDict\n",
    "wnut = datasets.load_dataset(\"wnut_17\")\n",
    "\n",
    "CONLL_NER_TAGS = conll['train'].features['ner_tags'].feature.names # всего 9 тегов\n",
    "WNUT_NER_TAGS = wnut['train'].features['ner_tags'].feature.names\n",
    "\n",
    "label_mapping = {\n",
    "    'O': 'O',\n",
    "    'B-location': 'B-LOC',\n",
    "    'I-location': 'I-LOC',\n",
    "    'B-group': 'B-ORG',\n",
    "    'B-corporation': 'B-ORG',\n",
    "    'B-person': 'B-PER',\n",
    "    'B-creative-work': 'B-MISC',\n",
    "    'B-product': 'B-MISC',\n",
    "    'I-person': 'I-PER',\n",
    "    'I-creative-work': 'I-MISC',\n",
    "    'I-corporation': 'I-ORG',\n",
    "    'I-group': 'I-ORG',\n",
    "    'I-product': 'I-MISC'\n",
    "}\n",
    "\n",
    "label2id_conll= {'O': 0,\n",
    " 'B-PER': 1,\n",
    " 'I-PER': 2,\n",
    " 'B-ORG': 3,\n",
    " 'I-ORG': 4,\n",
    " 'B-LOC': 5,\n",
    " 'I-LOC': 6,\n",
    " 'B-MISC': 7,\n",
    " 'I-MISC': 8}\n",
    "\n",
    "labelindexmapping = {WNUT_NER_TAGS.index(k):CONLL_NER_TAGS.index(v) for k, v in label_mapping.items()}\n",
    "\n",
    "import data_prep\n",
    "\n",
    "converted_wnut = wnut.map(lambda x: data_prep.convert_label_sequence(x, labelindexmapping))\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (pipeline, \n",
    "        AutoModelForTokenClassification, AutoTokenizer, \n",
    "        BertForTokenClassification, BertTokenizer)\n",
    "\n",
    "import importlib\n",
    "import data_prep\n",
    "\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "\n",
    "assert tokenizer.decode(test_result['input_ids']) == '[CLS] His name is Jerry Abrahamson [SEP]'\n",
    "\n",
    "                                     #CLS     His  name is    Jerry    Abraham   ##son      SEP\n",
    "assert test_result['text_labels'] == ['O'] + [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\",  \"I-PER\"] + [\"O\"]\n",
    "\n",
    "conll = conll.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "wnut = converted_wnut\n",
    "wnut = wnut.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "conll.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "wnut.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "\n",
    "input_keys = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "batch_size = 32\n",
    "wnut_train_dataloader = torch.utils.data.DataLoader(wnut[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "wnut_test_dataloader = torch.utils.data.DataLoader(wnut[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_train_dataloader = torch.utils.data.DataLoader(conll[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_test_dataloader = torch.utils.data.DataLoader(conll[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3669b59-c2ac-458c-a41b-a64fd8ffb249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "420d3ff0-5af4-4735-b175-cb4ce676beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 47, 9])\n",
      "[0.9770017266273499, 0.9972400665283203, 0.9488642811775208, 0.9996575713157654, 0.9515469074249268, 0.9970041513442993, 0.999862551689148, 0.999904215335846, 0.9905007481575012, 0.9993622303009033, 0.989292562007904, 0.9799342751502991, 0.9869406819343567, 0.9998921751976013, 0.9998874068260193, 0.9990612864494324, 0.9570358991622925, 0.9998361468315125, 0.9946780204772949, 0.9996030926704407, 0.9914760589599609, 0.9993963241577148, 0.9936320781707764, 0.999690592288971, 0.999440610408783, 0.9997891187667847, 0.9997739791870117, 0.9849514365196228, 0.9998698234558105, 0.9916747808456421, 0.9861404895782471, 0.9998873472213745]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(wnut_train_dataloader))\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                 token_type_ids=batch[\"token_type_ids\"],\n",
    "                 attention_mask=batch[\"attention_mask\"],\n",
    "                 labels=batch[\"labels\"], return_dict=True)\n",
    "\n",
    "print(outputs.logits.shape)\n",
    "attn_mask = batch['attention_mask']\n",
    "probs = torch.softmax(outputs.logits, dim=-1)\n",
    "max_probs = probs.max(dim=-1).values\n",
    "\n",
    "confidences = []\n",
    "for i in range(max_probs.shape[0]):\n",
    "    mask = attn_mask[i].bool()\n",
    "    conf = max_probs[i][mask].mean().item()\n",
    "    confidences.append(conf)\n",
    "\n",
    "print(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1a293fd-5345-46dc-b0f5-3bd189ac7716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c39402e0164d96a4b192e12d0f086d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_confidences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(wnut_train_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)             # [B, T, C]\n",
    "        max_probs = probs.max(dim=-1).values                      # [B, T]\n",
    "\n",
    "        for i in range(input_ids.size(0)):\n",
    "            real_token_mask = attention_mask[i].bool()            # [T]\n",
    "            conf = max_probs[i][real_token_mask].mean().item()\n",
    "            all_confidences.append(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33ba3553-3bf1-4acb-9fee-2b6d25992299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# убедимся, что длина совпадает\n",
    "assert len(all_confidences) == len(wnut[\"train\"])\n",
    "\n",
    "# добавим как новое поле\n",
    "wnut[\"train\"] = wnut[\"train\"].add_column(\"confidence\", all_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96c865f6-a568-4406-9a99-3581074dbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = np.array(all_confidences)\n",
    "threshold = np.percentile(confidences, 50)\n",
    "\n",
    "filtered_wnut = wnut[\"train\"].filter(lambda example: example[\"confidence\"] >= threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a57e62-1e66-4c70-ab08-6c34c25d165a",
   "metadata": {},
   "source": [
    "Датасет для файн-тюна готов.\n",
    "\n",
    "Как обычно загрузим базовую модель (без файн-тюна) и будем сравнивать результаты с ней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a76712a7-1f14-4022-8c05-239336540b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'text_labels', 'confidence'],\n",
       "    num_rows: 1697\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db43eff6-6e25-421d-a3d4-332fdd7cdd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем прокси-метки в датасет\n",
    "filtered_wnut_dataloader = torch.utils.data.DataLoader(filtered_wnut,        \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "\n",
    "# снова получим прокси-лейблы для топ 50 процентов самых уверенных примеров из wnut_train\n",
    "# получаем прокси-лейблы\n",
    "ner = NamedEntityPredictor(model_ft, tokenizer, id2label=id2label_conll)\n",
    "text_proxy_labels = []\n",
    "proxy_labels = []\n",
    "\n",
    "for batch in filtered_wnut_dataloader:\n",
    "    text_labels_batch = ner.predict(batch)['predicted_labels']  # list[list[str]]\n",
    "    text_proxy_labels.extend(text_labels_batch)\n",
    "\n",
    "    # переводим текстовые метки в индексы\n",
    "    index_batch = [\n",
    "        [label2id_conll[label] for label in seq]\n",
    "        for seq in text_labels_batch\n",
    "    ]\n",
    "    proxy_labels.extend(index_batch)\n",
    "\n",
    "# создаем новый датасет с заменой labels и text_labels\n",
    "filtered_wnut_proxy = filtered_wnut.remove_columns(['labels', 'text_labels']) \\\n",
    "                                   .add_column('labels', proxy_labels) \\\n",
    "                                   .add_column('text_labels', text_proxy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17cae7db-2947-4f4f-8530-5bb69734c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:05<00:00, 37.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:03<00:00, 22.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.15514242801288683 \t Test: 0.6412174041256492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.1428793407699189 \t Test: 0.6313148220012217\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.13230941449368164 \t Test: 0.6219541011033235\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.1233204874306891 \t Test: 0.6126955077971941\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.1158785819343648 \t Test: 0.6047563201483385\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.1098267687647821 \t Test: 0.5987705165221368\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.10517439774126598 \t Test: 0.5931387023057466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.1018688498847143 \t Test: 0.5897175774530128\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.09991647766100104 \t Test: 0.5874280808148561\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.09926836100781214 \t Test: 0.5866804956286041\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.75 (+0.00)  0.84 (+0.00)  0.79 (+0.00)  0.43 (-0.02)   \n",
      "MISC          0.56 (-0.02)  0.70 (+0.00)  0.62 (-0.01)  0.15 (-0.02)   \n",
      "ORG           0.72 (+0.01)  0.81 (-0.01)  0.76 (+0.00)  0.19 (+0.03)   \n",
      "PER           0.37 (+0.01)  0.57 (-0.01)  0.45 (+0.00)  0.32 (+0.02)   \n",
      "micro avg     0.59 (+0.01)  0.74 (+0.00)  0.65 (+0.00)  0.25 (+0.01)   \n",
      "macro avg     0.60 (+0.00)  0.73 (-0.01)  0.66 (+0.00)  0.27 (+0.00)   \n",
      "weighted avg  0.61 (+0.00)  0.74 (+0.00)  0.66 (+0.00)  0.27 (+0.01)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.52 (+0.01)  0.47 (-0.01)  \n",
      "MISC          0.22 (+0.00)  0.18 (-0.01)  \n",
      "ORG           0.27 (-0.03)  0.22 (+0.01)  \n",
      "PER           0.28 (-0.02)  0.30 (+0.00)  \n",
      "micro avg     0.30 (-0.01)  0.27 (+0.00)  \n",
      "macro avg     0.32 (-0.01)  0.29 (+0.00)  \n",
      "weighted avg  0.30 (-0.01)  0.28 (+0.00)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.27 (-0.00)\n",
      "pr     |    0.25 (+0.01)\n",
      "rec    |    0.30 (-0.01)\n",
      "acc    |    0.93 (+0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, param in model_ft.named_parameters():\n",
    "#     if name.startswith(\"bert.embeddings\") or name.startswith(\"bert.encoder.layer.0\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.1\") or name.startswith(\"bert.encoder.layer.2\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.3\") or name.startswith(\"bert.encoder.layer.4\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.5\") or name.startswith(\"bert.encoder.layer.6\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.7\"):\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = filtered_wnut_proxy\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9254b54f-88fe-4af9-aca2-27e9cc29df15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.15514242801288683,\n",
       "  0.1428793407699189,\n",
       "  0.13230941449368164,\n",
       "  0.1233204874306891,\n",
       "  0.1158785819343648,\n",
       "  0.1098267687647821,\n",
       "  0.10517439774126598,\n",
       "  0.1018688498847143,\n",
       "  0.09991647766100104,\n",
       "  0.09926836100781214],\n",
       " 'test_loss': [0.6412174041256492,\n",
       "  0.6313148220012217,\n",
       "  0.6219541011033235,\n",
       "  0.6126955077971941,\n",
       "  0.6047563201483385,\n",
       "  0.5987705165221368,\n",
       "  0.5931387023057466,\n",
       "  0.5897175774530128,\n",
       "  0.5874280808148561,\n",
       "  0.5866804956286041],\n",
       " 'f1': [0.27412008281573497,\n",
       "  0.2732971165900543,\n",
       "  0.2745263157894737,\n",
       "  0.27184466019417475,\n",
       "  0.27299703264094954,\n",
       "  0.27091295116772823,\n",
       "  0.2716468590831918,\n",
       "  0.2711432214194645,\n",
       "  0.27125850340136054,\n",
       "  0.2713738834538494],\n",
       " 'pr': [0.24775449101796407,\n",
       "  0.24885844748858446,\n",
       "  0.2515432098765432,\n",
       "  0.2496124031007752,\n",
       "  0.2515625,\n",
       "  0.25,\n",
       "  0.2505873140172279,\n",
       "  0.25039246467817894,\n",
       "  0.25058915946582877,\n",
       "  0.2507861635220126],\n",
       " 'rec': [0.30676552363299353,\n",
       "  0.3030583873957368,\n",
       "  0.30213160333642264,\n",
       "  0.29842446709916587,\n",
       "  0.29842446709916587,\n",
       "  0.29564411492122333,\n",
       "  0.2965708989805375,\n",
       "  0.29564411492122333,\n",
       "  0.29564411492122333,\n",
       "  0.29564411492122333],\n",
       " 'acc': [0.9285646858922925,\n",
       "  0.9291074715627508,\n",
       "  0.9295794591022797,\n",
       "  0.9297210553641384,\n",
       "  0.9298862510029735,\n",
       "  0.9299334497569264,\n",
       "  0.9300042478878557,\n",
       "  0.9299570491339029,\n",
       "  0.9300042478878557,\n",
       "  0.9300278472648322]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_metrics_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94ee7b-2b13-4848-9e07-7f1c0c7a3fa6",
   "metadata": {},
   "source": [
    "Если заморозить первые 8 слоев BERTа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cb9d262-1d15-4fac-8d45-293f75bff83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:05<00:00, 37.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:03<00:00, 22.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.1182837890141217 \t Test: 0.5857875069350372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.052627770420572956 \t Test: 0.47295676907639445\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.03533619093419288 \t Test: 0.47353565012231286\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.02697076203103534 \t Test: 0.4831461395010536\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.0219786444958517 \t Test: 0.4924837188956178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.018449030451941146 \t Test: 0.5057527014502773\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.016246403792163654 \t Test: 0.5116289835653187\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.014924626295736591 \t Test: 0.5142114935097871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.014126561844566942 \t Test: 0.5164081595930052\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:04<00:00, 22.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.01387225919692542 \t Test: 0.5179275454562388\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.19 (-0.56)  0.34 (-0.50)  0.25 (-0.54)  0.02 (-0.43)   \n",
      "MISC          0.19 (-0.39)  0.69 (-0.01)  0.30 (-0.33)  0.10 (-0.07)   \n",
      "ORG           0.14 (-0.57)  0.10 (-0.72)  0.12 (-0.64)  0.01 (-0.15)   \n",
      "PER           0.00 (-0.36)  0.00 (-0.58)  0.00 (-0.45)  0.00 (-0.30)   \n",
      "micro avg     0.14 (-0.44)  0.22 (-0.52)  0.17 (-0.48)  0.05 (-0.19)   \n",
      "macro avg     0.13 (-0.47)  0.28 (-0.46)  0.17 (-0.49)  0.03 (-0.24)   \n",
      "weighted avg  0.12 (-0.49)  0.22 (-0.52)  0.15 (-0.51)  0.03 (-0.23)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.07 (-0.44)  0.03 (-0.45)  \n",
      "MISC          0.24 (+0.02)  0.14 (-0.05)  \n",
      "ORG           0.00 (-0.30)  0.01 (-0.20)  \n",
      "PER           0.00 (-0.30)  0.00 (-0.30)  \n",
      "micro avg     0.07 (-0.24)  0.06 (-0.21)  \n",
      "macro avg     0.08 (-0.25)  0.05 (-0.24)  \n",
      "weighted avg  0.07 (-0.24)  0.04 (-0.24)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.06 (-0.21)\n",
      "pr     |    0.05 (-0.19)\n",
      "rec    |    0.07 (-0.24)\n",
      "acc    |    0.91 (-0.02)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, param in model_ft.named_parameters():\n",
    "#     if name.startswith(\"bert.embeddings\") or name.startswith(\"bert.encoder.layer.0\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.1\") or name.startswith(\"bert.encoder.layer.2\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.3\") or name.startswith(\"bert.encoder.layer.4\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.5\") or name.startswith(\"bert.encoder.layer.6\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.7\"):\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "# optimizer = AdamW(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = filtered_wnut_proxy\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7918230-233b-4f93-863d-7fbb774644a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.1182837890141217,\n",
       "  0.052627770420572956,\n",
       "  0.03533619093419288,\n",
       "  0.02697076203103534,\n",
       "  0.0219786444958517,\n",
       "  0.018449030451941146,\n",
       "  0.016246403792163654,\n",
       "  0.014924626295736591,\n",
       "  0.014126561844566942,\n",
       "  0.01387225919692542],\n",
       " 'test_loss': [0.5857875069350372,\n",
       "  0.47295676907639445,\n",
       "  0.47353565012231286,\n",
       "  0.4831461395010536,\n",
       "  0.4924837188956178,\n",
       "  0.5057527014502773,\n",
       "  0.5116289835653187,\n",
       "  0.5142114935097871,\n",
       "  0.5164081595930052,\n",
       "  0.5179275454562388],\n",
       " 'f1': [0.2609078889378581,\n",
       "  0.18713450292397663,\n",
       "  0.13495864170657382,\n",
       "  0.10855949895615866,\n",
       "  0.09551255660765746,\n",
       "  0.0813615608136156,\n",
       "  0.07434640522875817,\n",
       "  0.06874241811564902,\n",
       "  0.06360708534621577,\n",
       "  0.06131504638967325],\n",
       " 'pr': [0.24873949579831933,\n",
       "  0.18181818181818182,\n",
       "  0.1272577996715928,\n",
       "  0.09878419452887538,\n",
       "  0.08592592592592592,\n",
       "  0.07368421052631578,\n",
       "  0.0664718772826881,\n",
       "  0.06097560975609756,\n",
       "  0.056227758007117434,\n",
       "  0.054285714285714284],\n",
       " 'rec': [0.2743280815569972,\n",
       "  0.1927710843373494,\n",
       "  0.14365152919369786,\n",
       "  0.12048192771084337,\n",
       "  0.10750695088044486,\n",
       "  0.09082483781278962,\n",
       "  0.08433734939759036,\n",
       "  0.07877664504170528,\n",
       "  0.0732159406858202,\n",
       "  0.07043558850787766],\n",
       " 'acc': [0.9298154528720441,\n",
       "  0.9254023693774485,\n",
       "  0.9218860622079577,\n",
       "  0.9189125407089253,\n",
       "  0.9165998017652334,\n",
       "  0.9153018360315288,\n",
       "  0.9141454665596829,\n",
       "  0.9130126964648133,\n",
       "  0.9126823051871431,\n",
       "  0.9126587058101666]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_metrics_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccba18b6-c15b-4f6c-b2a3-7b4a2f6be3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    0,    0,    0,    0,    0,    0,    3,    4,    4,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100], device='cuda:0')\n",
      "tensor([   0,    0,    1,    2,    2,    2,    0,    1,    2,    2,    0,    0,\n",
      "           0,    7,    8,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
      "        -100], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# сравним настоящие и прокси метки на одном примере\n",
    "filtered_wnut_dataloader_proxy = torch.utils.data.DataLoader(filtered_wnut_proxy,        \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "\n",
    "print(next(iter(filtered_wnut_dataloader))['labels'][0])\n",
    "print(next(iter(filtered_wnut_dataloader_proxy))['labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a63fb-c6fa-4b85-aaf3-1bd5ac9e865f",
   "metadata": {},
   "source": [
    "Сравнение с обычным файн-тюном на всем трейне wnut при тех же гиперпараметрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0b8e66c-072c-435f-a166-a2172a416ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:05<00:00, 37.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:03<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.4180905382349112 \t Test: 0.6289433943636623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.3914361883685968 \t Test: 0.5983543199153594\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.36940671482654375 \t Test: 0.571914682601705\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.35110347286284527 \t Test: 0.5492657277687096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.33629396404618417 \t Test: 0.5305876625172886\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.32464878783326284 \t Test: 0.5156259777737252\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.31581655430181005 \t Test: 0.5040447684349837\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.3095226195231776 \t Test: 0.49577684314162646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.3058671420701196 \t Test: 0.49096254066184714\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.30464231571025935 \t Test: 0.4893680982015751\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.76 (+0.01)  0.84 (+0.00)  0.80 (+0.01)  0.49 (+0.04)   \n",
      "MISC          0.57 (-0.01)  0.69 (-0.01)  0.63 (+0.00)  0.17 (+0.00)   \n",
      "ORG           0.72 (+0.01)  0.81 (-0.01)  0.76 (+0.00)  0.19 (+0.03)   \n",
      "PER           0.38 (+0.02)  0.59 (+0.01)  0.46 (+0.01)  0.33 (+0.03)   \n",
      "micro avg     0.59 (+0.01)  0.74 (+0.00)  0.66 (+0.01)  0.27 (+0.03)   \n",
      "macro avg     0.61 (+0.01)  0.73 (-0.01)  0.66 (+0.00)  0.29 (+0.02)   \n",
      "weighted avg  0.62 (+0.01)  0.74 (+0.00)  0.67 (+0.01)  0.28 (+0.02)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.51 (+0.00)  0.50 (+0.02)  \n",
      "MISC          0.19 (-0.03)  0.18 (-0.01)  \n",
      "ORG           0.27 (-0.03)  0.23 (+0.02)  \n",
      "PER           0.28 (-0.02)  0.30 (+0.00)  \n",
      "micro avg     0.29 (-0.02)  0.28 (+0.01)  \n",
      "macro avg     0.31 (-0.02)  0.30 (+0.01)  \n",
      "weighted avg  0.29 (-0.02)  0.28 (+0.00)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.28 (+0.01)\n",
      "pr     |    0.27 (+0.03)\n",
      "rec    |    0.29 (-0.02)\n",
      "acc    |    0.93 (+0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# берем не топ 50 процентов по уверенности модели, а просто 50 процентов рандомных примеров из wnut train\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random_indices = random.sample(range(len(wnut[\"train\"])), len(filtered_wnut))\n",
    "wnut_train_random_subs = [wnut[\"train\"][i] for i in random_indices]\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, param in model_ft.named_parameters():\n",
    "#     if name.startswith(\"bert.embeddings\") or name.startswith(\"bert.encoder.layer.0\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.1\") or name.startswith(\"bert.encoder.layer.2\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.3\") or name.startswith(\"bert.encoder.layer.4\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.5\") or name.startswith(\"bert.encoder.layer.6\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.7\"):\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut_train_random_subs\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c586826-9637-400d-888f-6d490a601629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.4180905382349112,\n",
       "  0.3914361883685968,\n",
       "  0.36940671482654375,\n",
       "  0.35110347286284527,\n",
       "  0.33629396404618417,\n",
       "  0.32464878783326284,\n",
       "  0.31581655430181005,\n",
       "  0.3095226195231776,\n",
       "  0.3058671420701196,\n",
       "  0.30464231571025935],\n",
       " 'test_loss': [0.6289433943636623,\n",
       "  0.5983543199153594,\n",
       "  0.571914682601705,\n",
       "  0.5492657277687096,\n",
       "  0.5305876625172886,\n",
       "  0.5156259777737252,\n",
       "  0.5040447684349837,\n",
       "  0.49577684314162646,\n",
       "  0.49096254066184714,\n",
       "  0.4893680982015751],\n",
       " 'f1': [0.275290215588723,\n",
       "  0.2753684210526316,\n",
       "  0.27630453378956377,\n",
       "  0.27999999999999997,\n",
       "  0.2789886660854403,\n",
       "  0.27880386983289357,\n",
       "  0.2800177226406734,\n",
       "  0.2803571428571428,\n",
       "  0.2789450156459544,\n",
       "  0.27931960608773504],\n",
       " 'pr': [0.2490622655663916,\n",
       "  0.2523148148148148,\n",
       "  0.2565528196981732,\n",
       "  0.26371826371826373,\n",
       "  0.26337448559670784,\n",
       "  0.26527196652719665,\n",
       "  0.26825127334465193,\n",
       "  0.2704565030146425,\n",
       "  0.2694300518134715,\n",
       "  0.2701298701298701],\n",
       " 'rec': [0.3076923076923077,\n",
       "  0.3030583873957368,\n",
       "  0.29935125115848005,\n",
       "  0.29842446709916587,\n",
       "  0.2965708989805375,\n",
       "  0.293790546802595,\n",
       "  0.2928637627432808,\n",
       "  0.29101019462465244,\n",
       "  0.2891566265060241,\n",
       "  0.2891566265060241],\n",
       " 'acc': [0.9287770802850804,\n",
       "  0.9296266578562326,\n",
       "  0.9302874404115732,\n",
       "  0.9305234341813376,\n",
       "  0.9304526360504083,\n",
       "  0.9306414310662199,\n",
       "  0.930806626705055,\n",
       "  0.9309954217208666,\n",
       "  0.9310662198517959,\n",
       "  0.9311134186057488]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_metrics_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c044b0f-2008-4d24-8650-1ec9eb5c594d",
   "metadata": {},
   "source": [
    "Файн-тюн на истинных метках ожидаемо оказался лучше файн-тюна на прокси-метках"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee54d19-bd7a-408d-9c20-6e3641b2c689",
   "metadata": {},
   "source": [
    "##### 3.2.2 Unsupervised pretraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1d9c77-9baf-40df-bd9d-b69435ed5a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Dataset' has no attribute 'from_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 9\u001b[0m reddit_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m(data)\n\u001b[1;32m     10\u001b[0m reddit_dataset\u001b[38;5;241m=\u001b[39m reddit_dataset\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'from_list'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('reddit_sample.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "from datasets import Dataset\n",
    "reddit_dataset = Dataset.from_list(data)\n",
    "reddit_dataset= reddit_dataset.rename_column(\"words\", \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef721adc-48a7-491b-a720-51f4a9057f25",
   "metadata": {},
   "source": [
    "Аналогично 3.1.2 отберем 100 000 примеров, наиболее похожих на wnut['train'] и дообучим BERT на задачу masked LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41d250-a6ae-487f-aef4-c0224290d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bd22f643-6c2c-4fea-9c9b-4b2a052e85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['tokens'] # называется токены, но в датасете это просто списки слов\n",
    "columns_to_remove = [col for col in wnut['train'].column_names if col not in columns_to_keep]\n",
    "\n",
    "wnut_train_new = wnut['train'].remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "93bee0c6-9791-4758-9e50-ab7e19fa0bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eb07392f-b26d-4f70-9fa6-31c4fcec84aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens'],\n",
       "    num_rows: 500000\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a0764dbd-46c1-4d1c-bfd2-b73cbd8d271d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5af39b22ea48c8b06be261500e47a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "reddit_dataset = reddit_dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "114b96e3-3356-45a9-bf05-9315e6c6ab12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8595bd3340834fbbb270e65de581adc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wnut_train_new = wnut_train_new.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cd5f7dfd-d235-4e08-bda7-b97ef0c815e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94405fb7404047a982519b3b43596394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1620da5f264d411aa23d68f8c3b56107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reddit_dataset = reddit_dataset.map(lambda x: {\"domain_label\": int(0)})\n",
    "wnut_train_new = wnut_train_new.map(lambda x: {\"domain_label\": int(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "567a5cde-f909-4e18-ac34-4f8ecf7b3ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'domain_label'],\n",
       "    num_rows: 500000\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6977bb96-e935-46aa-b058-50fa932c32c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'domain_label'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "50a5e4b0-ce9e-49c2-97ea-0577d2f40d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined = concatenate_datasets([reddit_dataset, wnut_train_new]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d788bb48-ec32-4ad3-88d0-233aabe0e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4e968566-3394-4b2c-ab74-3f4fb3501cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['domain_label'][500_001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "676900ce-c229-495e-a350-36f206f3e707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ff34e614-2909-4b9b-9515-018012145c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ex in enumerate(combined):\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if ex.get(key) is None:\n",
    "            print(f\"Пропущено значение в примере {i}: {key} = None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2bbb822b-ee51-4572-a5cf-3b12f3ab2240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4523fb5c06374bf096e1cec16ee138c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting embeddings:   0%|          | 0/15732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "import data_prep\n",
    "from data_prep import PadSequence\n",
    "\n",
    "collate_fn = PadSequence(['input_ids','token_type_ids','attention_mask'])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    combined,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "batch\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "    batch = {k: batch[k].to(device) for k in ['input_ids','token_type_ids','attention_mask']}\n",
    "    with torch.no_grad():\n",
    "        outputs=model(**batch)\n",
    "        cls_emb = model(**batch).last_hidden_state[:, 0] # (batch_size, emb_size=768)\n",
    "    all_embeddings.append(cls_emb.cpu())\n",
    "\n",
    "all_embeddings = torch.cat(all_embeddings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c2aa28c8-f191-4b0b-98dc-c2f08220c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.add_column(\"cls_embedding\", all_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "078f902a-31c0-44df-ac52-8ee2df3654e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'domain_label', 'cls_embedding'],\n",
       "    num_rows: 503394\n",
       "})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eb62adb6-0cf8-4477-ba0e-281f510965c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44252914f7f34612bacc358647b4f2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training classifiers:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "k = 5\n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "wnut_scores = np.zeros(len(combined))\n",
    "X = combined[\"cls_embedding\"].cpu().numpy()\n",
    "Y = combined[\"domain_label\"].cpu().numpy()\n",
    "\n",
    "indices = np.arange(len(combined))\n",
    "for i, (train_idxs, test_idxs) in enumerate(tqdm(kf.split(X, Y), total=k, desc=\"Training classifiers\")):\n",
    "    X_train, X_test = X[train_idxs], X[test_idxs]\n",
    "    Y_train = Y[train_idxs]\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "    clf.fit(X_train, Y_train)\n",
    "    probas = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    wnut_scores[test_idxs] = probas\n",
    "    \n",
    "combined = combined.add_column(\"wnut_score\", wnut_scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e0ee0501-4baa-4f97-8e93-bbaa8c517178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e26c4c63be4e08b7643686f77677cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/503394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# отбираем топ 20 % объектов из reddit_dataset по схожести с датасетом с wnut\n",
    "\n",
    "reddit_subset = combined.filter(lambda x: x[\"domain_label\"] == 0)\n",
    "\n",
    "reddit_sorted = reddit_subset.sort(\"wnut_score\", reverse=True) \n",
    "\n",
    "# построить гистограмму с wnut_score\n",
    "\n",
    "top_20_count = int(0.20 * len(reddit_sorted))\n",
    "top_20_reddit = reddit_sorted.select(range(top_20_count))\n",
    "\n",
    "# top_20_reddit = top_20_reddit.remove_columns([\n",
    "#     col for col in top_20_reddit.column_names if col not in ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a8f2909e-5d50-4dfd-906b-3a45161af478",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_examples = top_20_reddit['tokens']\n",
    "\n",
    "import json\n",
    "with open(\"sel_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sel_examples, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8d10e359-6e38-465a-909a-21d4395fba2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1122,   134, 27322, 17482,  2340,   119,  8916,   102])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_reddit['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb89210-2a55-4ddf-915d-d231320c2cdd",
   "metadata": {},
   "source": [
    "Теперь есть топ 20 % из 500 000 примеров для файн-тюна самого BERTа на задаче masked LM.\n",
    "\n",
    "Код для файн-тюнинга BERTа на задаче MLM лежит в ноутбуке mlm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb07c949-2efa-4cc6-834c-80dcbbe8be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil_tomlv/miniconda3/envs/lmenv/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:19: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.22.3)\n",
      "  from scipy.stats import pearsonr, spearmanr\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import typing as tp\n",
    "import inspect\n",
    "import data_prep\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch \n",
    "from transformers import (pipeline, \n",
    "        AutoModelForTokenClassification, AutoTokenizer, \n",
    "        BertForTokenClassification, BertTokenizer)\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW, BertTokenizer, BertForMaskedLM, set_seed, BertTokenizerFast\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import data_prep\n",
    "import importlib\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "conll = load_from_disk(\"datasets/conll2003\")\n",
    "wnut = load_from_disk(\"datasets/wnut_17\")\n",
    "\n",
    "CONLL_NER_TAGS = conll['train'].features['ner_tags'].feature.names # всего 9 тегов\n",
    "WNUT_NER_TAGS = wnut['train'].features['ner_tags'].feature.names\n",
    "\n",
    "label_mapping = {\n",
    "    'O': 'O',\n",
    "    'B-location': 'B-LOC',\n",
    "    'I-location': 'I-LOC',\n",
    "    'B-group': 'B-ORG',\n",
    "    'B-corporation': 'B-ORG',\n",
    "    'B-person': 'B-PER',\n",
    "    'B-creative-work': 'B-MISC',\n",
    "    'B-product': 'B-MISC',\n",
    "    'I-person': 'I-PER',\n",
    "    'I-creative-work': 'I-MISC',\n",
    "    'I-corporation': 'I-ORG',\n",
    "    'I-group': 'I-ORG',\n",
    "    'I-product': 'I-MISC'\n",
    "}\n",
    "\n",
    "labelindexmapping = {WNUT_NER_TAGS.index(k):CONLL_NER_TAGS.index(v) for k, v in label_mapping.items()}\n",
    "\n",
    "converted_wnut = wnut.map(lambda x: data_prep.convert_label_sequence(x, labelindexmapping))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "label2id_conll= {'O': 0,\n",
    " 'B-PER': 1,\n",
    " 'I-PER': 2,\n",
    " 'B-ORG': 3,\n",
    " 'I-ORG': 4,\n",
    " 'B-LOC': 5,\n",
    " 'I-LOC': 6,\n",
    " 'B-MISC': 7,\n",
    " 'I-MISC': 8\n",
    "}\n",
    "\n",
    "id2label_conll = {v : k for k, v in label2id_conll.items()}\n",
    "\n",
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "\n",
    "assert tokenizer.decode(test_result['input_ids']) == '[CLS] His name is Jerry Abrahamson [SEP]'\n",
    "\n",
    "                                     #CLS     His  name is    Jerry    Abraham   ##son      SEP\n",
    "assert test_result['text_labels'] == ['O'] + [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\",  \"I-PER\"] + [\"O\"]\n",
    "\n",
    "conll = conll.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "wnut = converted_wnut\n",
    "wnut = wnut.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "conll.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "wnut.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "\n",
    "input_keys = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "batch_size = 16\n",
    "wnut_train_dataloader = torch.utils.data.DataLoader(wnut[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "wnut_test_dataloader = torch.utils.data.DataLoader(wnut[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_train_dataloader = torch.utils.data.DataLoader(conll[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_test_dataloader = torch.utils.data.DataLoader(conll[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaeab9fc-7f25-4bbc-95bf-98445cbde02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 95.293, 'PER': 1.485, 'ORG': 1.068, 'LOC': 1.125, 'MISC': 1.028}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = {k : 0 for k in label2id_conll.keys()}\n",
    "for example in wnut['train']['text_labels']:\n",
    "    for label in example:\n",
    "        counter[label] += 1\n",
    "\n",
    "total=0\n",
    "for count in counter.values():\n",
    "    total+=count\n",
    "\n",
    "freqs = {\n",
    "    \"O\" : counter['O']/total*100,\n",
    "    \"PER\" : (counter['B-PER']+counter['I-PER'])/total*100,\n",
    "    \"ORG\" : (counter['B-ORG']+counter['I-ORG'])/total*100,\n",
    "    \"LOC\" : (counter['B-LOC']+counter['I-LOC'])/total*100,\n",
    "    \"MISC\" : (counter['B-MISC']+counter['I-MISC'])/total*100  \n",
    "}\n",
    "\n",
    "freqs = {k : round(v,3) for k, v in freqs.items()}\n",
    "\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524e79b3-5a1e-4513-a7bc-e2d053dc675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, get_scheduler, BertTokenizerFast\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_ # выполняет обрезку градиентов для избежания их взрыва при обучении\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import importlib\n",
    "import data_prep\n",
    "import model_utils\n",
    "importlib.reload(data_prep)\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "from model_utils import train_eval_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e34aed6-da12-4b4e-9a5a-3ee93add83f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:04<00:00, 53.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:01<00:00, 41.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 55.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.41513655572569985 \t Test: 0.6117660495234124\n",
      "F1: 0.27580442958629336\n",
      "LR: [0.0]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.75 (+0.00)  0.84 (+0.00)  0.79 (+0.00)  0.46 (+0.01)   \n",
      "MISC          0.58 (+0.00)  0.70 (+0.00)  0.64 (+0.01)  0.18 (+0.01)   \n",
      "ORG           0.71 (+0.00)  0.82 (+0.00)  0.76 (+0.00)  0.18 (+0.02)   \n",
      "PER           0.37 (+0.01)  0.58 (+0.00)  0.45 (+0.00)  0.30 (+0.00)   \n",
      "micro avg     0.58 (+0.00)  0.74 (+0.00)  0.65 (+0.00)  0.25 (+0.01)   \n",
      "macro avg     0.60 (+0.00)  0.73 (-0.01)  0.66 (+0.00)  0.28 (+0.01)   \n",
      "weighted avg  0.61 (+0.00)  0.74 (+0.00)  0.67 (+0.01)  0.26 (+0.00)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.51 (+0.00)  0.48 (+0.00)  \n",
      "MISC          0.22 (+0.00)  0.19 (+0.00)  \n",
      "ORG           0.30 (+0.00)  0.22 (+0.01)  \n",
      "PER           0.29 (-0.01)  0.30 (+0.00)  \n",
      "micro avg     0.31 (+0.00)  0.28 (+0.01)  \n",
      "macro avg     0.33 (+0.00)  0.30 (+0.01)  \n",
      "weighted avg  0.31 (+0.00)  0.28 (+0.00)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.28 (+0.00)\n",
      "pr     |    0.25 (+0.01)\n",
      "rec    |    0.31 (-0.00)\n",
      "acc    |    0.93 (+0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 1\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33902aff-e5bb-44f0-bae5-4941b9240cd2",
   "metadata": {},
   "source": [
    "Если использовать continued pre-trained модель, полученную в ноутбуке mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9d5c98-394a-4caa-8c47-787a354b7352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1888fcab-41d7-472e-99ac-cf982e8c879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler.get_last_lr()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8269b1-c661-4500-b34f-a1a6948bb27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 32.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 41/41 [00:02<00:00, 20.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 2.297998386008717 \t Test: 2.2888116371340868\n",
      "F1: 0.004228444420350743\n",
      "LR: [4.000000000000001e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 2.203958834443137 \t Test: 2.1939932544056964\n",
      "F1: 0.004549904970339227\n",
      "LR: [8.000000000000001e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 2.0665758235432277 \t Test: 2.054909973609738\n",
      "F1: 0.004712953792561729\n",
      "LR: [1.2e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 1.9098785625439938 \t Test: 1.8952618372149583\n",
      "F1: 0.00573989225114546\n",
      "LR: [1.6000000000000003e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 1.7609850691857738 \t Test: 1.741776826905041\n",
      "F1: 0.0053984941042761764\n",
      "LR: [2e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 1.6465882051770933 \t Test: 1.6217689194330356\n",
      "F1: 0.004973291582243506\n",
      "LR: [1.9555555555555557e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 1.5679009718315624 \t Test: 1.5375050364471063\n",
      "F1: 0.0060479666319082385\n",
      "LR: [1.9111111111111113e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 1.5075281880726323 \t Test: 1.471891734658218\n",
      "F1: 0.007174887892376682\n",
      "LR: [1.866666666666667e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 1.4608769561642798 \t Test: 1.4207996100914189\n",
      "F1: 0.006734006734006735\n",
      "LR: [1.8222222222222224e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 1.4220910083467715 \t Test: 1.3780616667212509\n",
      "F1: 0.008320456436467371\n",
      "LR: [1.7777777777777777e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\t Train: 1.3896395057161277 \t Test: 1.3421144601775379\n",
      "F1: 0.0093953264273669\n",
      "LR: [1.7333333333333336e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\t Train: 1.3623148681961488 \t Test: 1.311716533288723\n",
      "F1: 0.010657623834322392\n",
      "LR: [1.688888888888889e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\t Train: 1.336644950314103 \t Test: 1.2834080225083886\n",
      "F1: 0.01096892138939671\n",
      "LR: [1.6444444444444444e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\t Train: 1.314097508091793 \t Test: 1.258618555417875\n",
      "F1: 0.010768477728830151\n",
      "LR: [1.6000000000000003e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\t Train: 1.2956154257337624 \t Test: 1.237824617362604\n",
      "F1: 0.011498470948012231\n",
      "LR: [1.555555555555556e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\t Train: 1.2761189870745222 \t Test: 1.2166996554630558\n",
      "F1: 0.01075926152341362\n",
      "LR: [1.5111111111111112e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\t Train: 1.260792805769733 \t Test: 1.1997447246458472\n",
      "F1: 0.010499328531314857\n",
      "LR: [1.4666666666666666e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\t Train: 1.2459394653266835 \t Test: 1.183523811945101\n",
      "F1: 0.0107095046854083\n",
      "LR: [1.4222222222222224e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\t Train: 1.2318542404709576 \t Test: 1.1682489674265792\n",
      "F1: 0.010742187500000002\n",
      "LR: [1.377777777777778e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\t Train: 1.2198492801078011 \t Test: 1.155209003425226\n",
      "F1: 0.010926308121889036\n",
      "LR: [1.3333333333333333e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21\t Train: 1.2121180841855914 \t Test: 1.1461906927387888\n",
      "F1: 0.011133970712816169\n",
      "LR: [1.288888888888889e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22\t Train: 1.2010867439697837 \t Test: 1.1344060287242983\n",
      "F1: 0.0111003861003861\n",
      "LR: [1.2444444444444446e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23\t Train: 1.1919237119015131 \t Test: 1.1244080968019439\n",
      "F1: 0.010624169986719787\n",
      "LR: [1.2e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24\t Train: 1.1833720051239585 \t Test: 1.115207768068081\n",
      "F1: 0.010842067220816768\n",
      "LR: [1.1555555555555556e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25\t Train: 1.1758379590845554 \t Test: 1.106869209103468\n",
      "F1: 0.011060351045924502\n",
      "LR: [1.1111111111111113e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26\t Train: 1.1698472978912782 \t Test: 1.100125897221449\n",
      "F1: 0.01080951237088638\n",
      "LR: [1.0666666666666667e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27\t Train: 1.16395807600467 \t Test: 1.0935640422309316\n",
      "F1: 0.010775862068965518\n",
      "LR: [1.0222222222222223e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28\t Train: 1.1572727755965473 \t Test: 1.086441226121856\n",
      "F1: 0.010775862068965518\n",
      "LR: [9.777777777777779e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29\t Train: 1.1518525899013625 \t Test: 1.0805499088473436\n",
      "F1: 0.010779734099892203\n",
      "LR: [9.333333333333334e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\t Train: 1.1455305302254508 \t Test: 1.073938704118496\n",
      "F1: 0.010814708002883922\n",
      "LR: [8.888888888888888e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31\t Train: 1.1398062817404204 \t Test: 1.0679508389496222\n",
      "F1: 0.010808214242824547\n",
      "LR: [8.444444444444446e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32\t Train: 1.1354145429958806 \t Test: 1.0632822179212802\n",
      "F1: 0.01079654510556622\n",
      "LR: [8.000000000000001e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33\t Train: 1.13111648492724 \t Test: 1.0587146950931083\n",
      "F1: 0.010792660990526443\n",
      "LR: [7.555555555555556e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34\t Train: 1.127135005509742 \t Test: 1.0544786395096197\n",
      "F1: 0.01077070368597415\n",
      "LR: [7.111111111111112e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35\t Train: 1.1231632516762922 \t Test: 1.0502874371482105\n",
      "F1: 0.010535137076499461\n",
      "LR: [6.666666666666667e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36\t Train: 1.119239939707462 \t Test: 1.0461736554052772\n",
      "F1: 0.010293237582286056\n",
      "LR: [6.222222222222223e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37\t Train: 1.1152500515786288 \t Test: 1.042055631556162\n",
      "F1: 0.010537660160459825\n",
      "LR: [5.777777777777778e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38\t Train: 1.112728335590006 \t Test: 1.0393170292784528\n",
      "F1: 0.01054144705318639\n",
      "LR: [5.333333333333334e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39\t Train: 1.1101536327433363 \t Test: 1.03656995877987\n",
      "F1: 0.010543973160795592\n",
      "LR: [4.888888888888889e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40\t Train: 1.1078985929489136 \t Test: 1.034173814261832\n",
      "F1: 0.010303102911225591\n",
      "LR: [4.444444444444444e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41\t Train: 1.1060171857058445 \t Test: 1.0321380612326831\n",
      "F1: 0.01005626720938585\n",
      "LR: [4.000000000000001e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42\t Train: 1.1037912162664896 \t Test: 1.0298121222635594\n",
      "F1: 0.010059880239520957\n",
      "LR: [3.555555555555556e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43\t Train: 1.1019601972303659 \t Test: 1.0279049349994194\n",
      "F1: 0.010293237582286056\n",
      "LR: [3.1111111111111116e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44\t Train: 1.1007805533498247 \t Test: 1.0266037147219589\n",
      "F1: 0.010047846889952153\n",
      "LR: [2.666666666666667e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\t Train: 1.0993181311081504 \t Test: 1.0250761785158298\n",
      "F1: 0.009808612440191386\n",
      "LR: [2.222222222222222e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46\t Train: 1.0984270450110747 \t Test: 1.0241173563933954\n",
      "F1: 0.009807439301518958\n",
      "LR: [1.777777777777778e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47\t Train: 1.0976196059556764 \t Test: 1.0232660101681221\n",
      "F1: 0.00980509386583762\n",
      "LR: [1.3333333333333334e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48\t Train: 1.0969741595125644 \t Test: 1.0225994601482298\n",
      "F1: 0.010040640688501076\n",
      "LR: [8.88888888888889e-07]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49\t Train: 1.0966147150948784 \t Test: 1.0222193627822689\n",
      "F1: 0.010040640688501076\n",
      "LR: [4.444444444444445e-07]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50\t Train: 1.0964217536917358 \t Test: 1.022026918283323\n",
      "F1: 0.010046645138141371\n",
      "LR: [0.0]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.00 (+0.00)  0.00 (-0.02)  0.00 (+0.00)  0.00 (+0.00)   \n",
      "MISC          0.00 (+0.00)  0.02 (+0.00)  0.01 (+0.01)  0.01 (+0.01)   \n",
      "ORG           0.04 (+0.04)  0.08 (+0.07)  0.05 (+0.05)  0.02 (+0.02)   \n",
      "PER           0.00 (-0.01)  0.00 (-0.16)  0.00 (-0.03)  0.00 (-0.01)   \n",
      "micro avg     0.01 (+0.00)  0.03 (-0.03)  0.02 (+0.01)  0.01 (+0.01)   \n",
      "macro avg     0.01 (+0.01)  0.03 (-0.02)  0.01 (+0.00)  0.01 (+0.01)   \n",
      "weighted avg  0.01 (+0.00)  0.03 (-0.03)  0.02 (+0.01)  0.01 (+0.01)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.01 (+0.00)  0.00 (+0.00)  \n",
      "MISC          0.07 (+0.05)  0.01 (+0.01)  \n",
      "ORG           0.06 (+0.00)  0.03 (+0.03)  \n",
      "PER           0.02 (-0.12)  0.01 (+0.00)  \n",
      "micro avg     0.04 (-0.03)  0.01 (+0.01)  \n",
      "macro avg     0.04 (-0.02)  0.01 (+0.01)  \n",
      "weighted avg  0.04 (-0.03)  0.01 (+0.00)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.01 (+0.01)\n",
      "pr     |    0.01 (+0.00)\n",
      "rec    |    0.04 (-0.04)\n",
      "acc    |    0.74 (+0.69)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "mlm_model = BertForMaskedLM.from_pretrained(\"model\")\n",
    "pretrained_bert = mlm_model.bert\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model_ft.bert = pretrained_bert\n",
    "model_ft.classifier.reset_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for name, param in model_ft.named_parameters():\n",
    "    if name.startswith(\"bert.embeddings\") or name.startswith(\"bert.encoder.layer.0\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.1\") or name.startswith(\"bert.encoder.layer.2\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.3\") or name.startswith(\"bert.encoder.layer.4\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.5\") or name.startswith(\"bert.encoder.layer.6\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.7\") or name.startswith(\"bert.encoder.layer.8\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.9\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# for name, param in model_ft.bert.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 50\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 32\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name=\"constant\",\n",
    "#     optimizer=optimizer\n",
    "# )\n",
    "\n",
    "# добавляем веса\n",
    "factor = 0.01\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b6fdcfd-842a-4553-9783-522d32d7ea5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261641f1-e027-4129-a8b0-c4ad2521592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 32.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 41/41 [00:02<00:00, 20.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 1.6596670061628396 \t Test: 1.624269433137847\n",
      "F1: 0.004522613065326633\n",
      "LR: [2e-05, 4.000000000000001e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 1.041628313398807 \t Test: 0.9723918205354272\n",
      "F1: 0.009520322226290737\n",
      "LR: [4e-05, 8.000000000000001e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.7939284995337513 \t Test: 0.6734083382094779\n",
      "F1: 0.012512441347931181\n",
      "LR: [6e-05, 1.2e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.6439093290088332 \t Test: 0.49279272265550567\n",
      "F1: 0.013678905687544998\n",
      "LR: [8e-05, 1.6000000000000003e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.5355829128595156 \t Test: 0.42445680062945296\n",
      "F1: 0.015322580645161291\n",
      "LR: [0.0001, 2e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.4061726326836604 \t Test: 0.37095753993929886\n",
      "F1: 0.01502253380070105\n",
      "LR: [9.777777777777778e-05, 1.9555555555555557e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.3891823785466568 \t Test: 0.37887974083423615\n",
      "F1: 0.01679816747263935\n",
      "LR: [9.555555555555557e-05, 1.9111111111111113e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.33065609331895535 \t Test: 0.37045543368269757\n",
      "F1: 0.016835016835016835\n",
      "LR: [9.333333333333334e-05, 1.866666666666667e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.3303770728161228 \t Test: 0.37666160776847746\n",
      "F1: 0.01669449081803005\n",
      "LR: [9.111111111111112e-05, 1.8222222222222224e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.25677192742744825 \t Test: 0.36603470328377513\n",
      "F1: 0.019552191737622203\n",
      "LR: [8.888888888888889e-05, 1.7777777777777777e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\t Train: 0.2074482659020496 \t Test: 0.3756830846754516\n",
      "F1: 0.018710324089542263\n",
      "LR: [8.666666666666667e-05, 1.7333333333333336e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\t Train: 0.20207760063931346 \t Test: 0.4037785173916235\n",
      "F1: 0.01745552198724404\n",
      "LR: [8.444444444444444e-05, 1.688888888888889e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([factor \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels)], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     56\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m CrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[0;32m---> 58\u001b[0m fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval_ner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mft_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid2label_conll\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hw_9/model_utils.py:222\u001b[0m, in \u001b[0;36mtrain_eval_ner\u001b[0;34m(model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn, ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size, id2label)\u001b[0m\n\u001b[1;32m    219\u001b[0m     predicted_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(ner\u001b[38;5;241m.\u001b[39mpredict(batch)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m ft_test_dataloader:\n\u001b[0;32m--> 222\u001b[0m     predicted_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(\u001b[43mner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    224\u001b[0m y_true_ft \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m ft_test_dataset]\n\u001b[1;32m    225\u001b[0m y_pred_ft \u001b[38;5;241m=\u001b[39m predicted_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/hw_9/model_utils.py:48\u001b[0m, in \u001b[0;36mNamedEntityPredictor.predict\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     42\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     43\u001b[0m                               token_type_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     44\u001b[0m                               attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     45\u001b[0m                               labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     46\u001b[0m                               return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model_output\u001b[38;5;241m.\u001b[39mlogits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# здесь индексы из model.config.id2label\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     50\u001b[0m label2id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2id\n\u001b[1;32m     51\u001b[0m model_id2label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# экспериментальная версия\n",
    "from transformers import BertTokenizerFast, BertForMaskedLM, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "mlm_model = BertForMaskedLM.from_pretrained(\"model\")\n",
    "pretrained_bert = mlm_model.bert\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model_ft.bert = pretrained_bert\n",
    "model_ft.classifier.reset_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for p in model_ft.bert.parameters():           \n",
    "    p.requires_grad = False\n",
    "for n, p in model_ft.bert.named_parameters():\n",
    "    if n.startswith((\"encoder.layer.8\",\"encoder.layer.9\",\"encoder.layer.10\", \"encoder.layer.11\")):\n",
    "        p.requires_grad = True\n",
    "for p in model_ft.classifier.parameters():     \n",
    "    p.requires_grad = True\n",
    "\n",
    "head_params = [p for n, p in model_ft.named_parameters() if \"classifier\" in n]\n",
    "bert_params = [p for n, p in model_ft.named_parameters()\n",
    "               if p.requires_grad and \"classifier\" not in n]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": head_params, \"lr\": 1e-4, \"weight_decay\": 0.01},\n",
    "        {\"params\": bert_params, \"lr\": 2e-5, \"weight_decay\": 0.01},\n",
    "    ],\n",
    "    betas=(0.9, 0.999), eps=1e-6\n",
    ")\n",
    "\n",
    "num_epochs = 50\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 32\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "factor = 0.01\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1e8dcea-9c43-439e-a172-f4031330abe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fc1d171-f261-482e-95be-e09ec3ab85b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    {'text' : 'His name is Jerry Abrahamson', 'ner_tags': [0, 0, 0, 1, 2]},\n",
    "    {'text' : 'London is big city', 'ner_tags' : [5, 0, 0, 0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98861385-94f6-41cc-a933-a4c97390d5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2010, 2171, 2003, 6128, 8181, 3385, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "test_sentence = test_sentences[idx]['text']\n",
    "\n",
    "tokenizer(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637c061-1a97-4d5e-a7b0-53706252f020",
   "metadata": {},
   "source": [
    "##### Тест NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17b243ee-298d-422e-b2ff-add65d063613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 4, None]\n",
      "[101, 2010, 2171, 2003, 6128, 8181, 3385, 102]\n",
      "['[CLS]', 'his', 'name', 'is', 'jerry', 'abraham', '##son', '[SEP]']\n",
      "True: O           Pred: O         \n",
      "True: O           Pred: O         \n",
      "True: O           Pred: O         \n",
      "True: O           Pred: O         \n",
      "True: B-PER       Pred: O         \n",
      "True: I-PER       Pred: O         \n",
      "True: I-PER       Pred: O         \n",
      "True: O           Pred: O         \n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import data_prep\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "idx = 0\n",
    "test_sentence = test_sentences[idx]['text']\n",
    "ner_tags = test_sentences[idx]['ner_tags']\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": ner_tags}\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "print(test_result.word_ids())\n",
    "print(test_result['input_ids'])\n",
    "print([tokenizer.decode(token) for token in test_result['input_ids']])\n",
    "\n",
    "test_batch = {k : [v] for k, v in test_result.items()}\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "batched_test_batch = defaultdict(list)\n",
    "\n",
    "for key, value in test_batch.items():\n",
    "    try:\n",
    "        batched_test_batch[key] = torch.tensor(value)\n",
    "    except ValueError:\n",
    "        batched_test_batch[key] = value\n",
    "\n",
    "for key in batched_test_batch:\n",
    "    if isinstance(batched_test_batch[key], torch.Tensor):\n",
    "        batched_test_batch[key] = batched_test_batch[key].to(device)\n",
    "\n",
    "import importlib\n",
    "import model_utils\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "from model_utils import NamedEntityPredictor\n",
    "\n",
    "ner_test = NamedEntityPredictor(model_ft, tokenizer, id2label_conll)\n",
    "# print(f\"Predicted:\\t {ner_test.predict(batched_test_batch)['predicted_labels']}\")\n",
    "# print(f\"True:\\t {batched_test_batch['text_labels']}\")\n",
    "\n",
    "pred = ner_test.predict(batched_test_batch)['predicted_labels']\n",
    "true = batched_test_batch['text_labels']\n",
    "for pred_seq, true_seq in zip(pred, true):\n",
    "    aligned = [(t, p) for t, p in zip(true_seq, pred_seq)]\n",
    "    for t, p in aligned:\n",
    "        print(f\"True: {t:<10}  Pred: {p:<10}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmenv",
   "language": "python",
   "name": "lmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
