{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e3da4b-1735-4600-9168-20bd56d6b588",
   "metadata": {},
   "source": [
    "#### 3.1 Supervised DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65e12c-ead8-470e-93e3-d9d9e752232a",
   "metadata": {},
   "source": [
    "##### 3.1.1 In-domain fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e0ae2-a745-428e-b15c-4e8c0c7e294b",
   "metadata": {},
   "source": [
    "Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3449ab-a809-49f3-bf35-9c31167e1150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil_tomlv/miniconda3/envs/lmenv/lib/python3.10/importlib/__init__.py:126: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.22.3)\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd79901503074cfdbedcf4adb2a144af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8af7ec48455481cb3517f2faf76eb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01304430af8b4c2a984b6ec8625eabf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f00b1d3e01d4255bf118906b797cc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d3b8d475f14f07a1827881f45736cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ff503378404db4971cc30217db107b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import typing as tp\n",
    "import inspect\n",
    "\n",
    "conll = datasets.load_dataset(\"conll2003\") # DatasetDict\n",
    "wnut = datasets.load_dataset(\"wnut_17\")\n",
    "\n",
    "conll.save_to_disk(\"datasets/conll2003\")\n",
    "wnut.save_to_disk(\"datasets/wnut_17\")\n",
    "\n",
    "CONLL_NER_TAGS = conll['train'].features['ner_tags'].feature.names # всего 9 тегов\n",
    "WNUT_NER_TAGS = wnut['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65a90fc-77e3-4df8-a69d-baf9c2c426b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6b52a1-6304-4e87-b2e5-26461f54f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1bb58e-1b52-4f3a-abb6-7046d9ba5b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONLL_NER_TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ddc48c-ca1d-40b9-8a85-76e60a0aeb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WNUT_NER_TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329e7480-41e6-4f8b-a7ab-5ed7242dfce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {\n",
    "    'O': 'O',\n",
    "    'B-location': 'B-LOC',\n",
    "    'I-location': 'I-LOC',\n",
    "    'B-group': 'B-ORG',\n",
    "    'B-corporation': 'B-ORG',\n",
    "    'B-person': 'B-PER',\n",
    "    'B-creative-work': 'B-MISC',\n",
    "    'B-product': 'B-MISC',\n",
    "    'I-person': 'I-PER',\n",
    "    'I-creative-work': 'I-MISC',\n",
    "    'I-corporation': 'I-ORG',\n",
    "    'I-group': 'I-ORG',\n",
    "    'I-product': 'I-MISC'\n",
    "}\n",
    "\n",
    "labelindexmapping = {WNUT_NER_TAGS.index(k):CONLL_NER_TAGS.index(v) for k, v in label_mapping.items()}\n",
    "\n",
    "import data_prep\n",
    "\n",
    "converted_wnut = wnut.map(lambda x: data_prep.convert_label_sequence(x, labelindexmapping))\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (pipeline, \n",
    "        AutoModelForTokenClassification, AutoTokenizer, \n",
    "        BertForTokenClassification, BertTokenizer)\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW, BertTokenizer, BertForMaskedLM, set_seed\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "set_seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import data_prep\n",
    "import importlib\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "label2id_conll= {'O': 0,\n",
    " 'B-PER': 1,\n",
    " 'I-PER': 2,\n",
    " 'B-ORG': 3,\n",
    " 'I-ORG': 4,\n",
    " 'B-LOC': 5,\n",
    " 'I-LOC': 6,\n",
    " 'B-MISC': 7,\n",
    " 'I-MISC': 8\n",
    "}\n",
    "\n",
    "id2label_conll = {v : k for k, v in label2id_conll.items()}\n",
    "\n",
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "# test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, model.config.label2id)\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "\n",
    "assert tokenizer.decode(test_result['input_ids']) == '[CLS] His name is Jerry Abrahamson [SEP]'\n",
    "\n",
    "                                     #CLS     His  name is    Jerry    Abraham   ##son      SEP\n",
    "assert test_result['text_labels'] == ['O'] + [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\",  \"I-PER\"] + [\"O\"]\n",
    "\n",
    "conll = conll.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "wnut = converted_wnut\n",
    "wnut = wnut.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "conll.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "wnut.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cef350f-4a5d-45d1-9bce-ee4ae7e8706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 4, None]\n",
      "[101, 1230, 1271, 1110, 5466, 7752, 2142, 102]\n",
      "['[CLS]', 'His', 'name', 'is', 'Jerry', 'Abraham', '##son', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import data_prep\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "print(test_result.word_ids())\n",
    "print(test_result['input_ids'])\n",
    "print([tokenizer.decode(token) for token in test_result['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdd5322-dbbb-477b-bb8a-50205e0f2283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1230, 1271, 1110, 5466, 7752, 2142, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'tokens': ['His', 'name', 'is', 'Jerry', 'Abrahamson'], 'ner_tags': [0, 0, 0, 1, 2], 'labels': [0, 0, 0, 0, 1, 2, 2, 0], 'text_labels': ['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec394c-8baf-4ce7-a78a-cf2578f7e78c",
   "metadata": {},
   "source": [
    "Теперь есть готовые датасеты conll и wnut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8d37d-a91b-4dd7-9ed6-a44d1283074e",
   "metadata": {},
   "source": [
    "Создадим даталоадеры для трейн и теста conll и wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690e880d-eabc-44b2-a449-3ecf2224a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "batch_size = 16\n",
    "wnut_train_dataloader = torch.utils.data.DataLoader(wnut[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "wnut_test_dataloader = torch.utils.data.DataLoader(wnut[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_train_dataloader = torch.utils.data.DataLoader(conll[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_test_dataloader = torch.utils.data.DataLoader(conll[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e5bed9-b6f3-4033-97db-74f7b37262e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "45\n",
      "14\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "batch=next(iter(wnut_test_dataloader))\n",
    "print(len(batch['ner_tags'][7]))\n",
    "print(len(batch['input_ids'][7]))\n",
    "print(len(batch['tokens'][7]))\n",
    "print(len(batch['labels'][7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7cc9c56-5cbc-4436-9ee9-107ae62945c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = {k : [v] for k, v in test_result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c776366-e727-4aeb-ae4e-f6e24bc2ecc9",
   "metadata": {},
   "source": [
    "Измерим метрики на базовой версии BERTа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e6d659-6d16-469b-b306-68a342ea70ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\" # дообучена на датасете CoNLL-2003\n",
    "model_base = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_base.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1bc7559-61c0-43b8-b038-05688a7c45c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1230, 1271, 1110, 5466, 7752, 2142, 102]],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]],\n",
       " 'tokens': [['His', 'name', 'is', 'Jerry', 'Abrahamson']],\n",
       " 'ner_tags': [[0, 0, 0, 1, 2]],\n",
       " 'labels': [[0, 0, 0, 0, 1, 2, 2, 0]],\n",
       " 'text_labels': [['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fed14692-28c9-4b81-974b-62d76cd79643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'input_ids': tensor([[ 101, 1230, 1271, 1110, 5466, 7752, 2142,  102]], device='cuda:0'),\n",
       "             'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       "             'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       "             'tokens': [['His', 'name', 'is', 'Jerry', 'Abrahamson']],\n",
       "             'ner_tags': tensor([[0, 0, 0, 1, 2]], device='cuda:0'),\n",
       "             'labels': tensor([[0, 0, 0, 0, 1, 2, 2, 0]], device='cuda:0'),\n",
       "             'text_labels': [['O',\n",
       "               'O',\n",
       "               'O',\n",
       "               'O',\n",
       "               'B-PER',\n",
       "               'I-PER',\n",
       "               'I-PER',\n",
       "               'O']]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тест, что все адекватно предсказывается\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "batched_test_batch = defaultdict(list)\n",
    "\n",
    "for key, value in test_batch.items():\n",
    "    try:\n",
    "        batched_test_batch[key] = torch.tensor(value)\n",
    "    except ValueError:\n",
    "        batched_test_batch[key] = value\n",
    "\n",
    "for key in batched_test_batch:\n",
    "    if isinstance(batched_test_batch[key], torch.Tensor):\n",
    "        batched_test_batch[key] = batched_test_batch[key].to(device)\n",
    "\n",
    "batched_test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f29e5265-c7c5-4d6c-aaf7-f8e025257c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b9021bd-e365-435b-b0ee-a90b59d14cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d337357-9a64-41a6-8ad3-3fecc9d6bd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted_labels': [['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']],\n",
       " 'loss': tensor(3.5653, device='cuda:0'),\n",
       " 'logits': tensor([[[ 6.8225, -0.2862, -1.0980, -0.3375, -1.1505, -1.0953, -1.7006,\n",
       "           -0.8111, -1.3737],\n",
       "          [ 9.3304, -1.1577, -2.1227, -0.1745, -1.8043, -1.7342, -2.2795,\n",
       "           -0.7647, -1.3315],\n",
       "          [ 9.7455, -1.0748, -1.7252, -0.2097, -1.3579, -2.4633, -1.7645,\n",
       "           -1.0774, -1.2969],\n",
       "          [ 9.3520, -1.2463, -1.9811, -0.2193, -1.5422, -2.2553, -1.8455,\n",
       "           -0.9268, -1.1779],\n",
       "          [-0.8416, -1.2910, -2.3588,  8.7858, -0.5230, -1.6059, -2.6656,\n",
       "           -0.3291, -1.6418],\n",
       "          [-0.7083, -1.2668, -1.3766, -1.0028,  8.9504, -1.9978, -0.7555,\n",
       "           -0.9198, -1.0510],\n",
       "          [ 1.7984, -1.4755, -1.4079, -0.3893,  6.5783, -2.9142, -1.2547,\n",
       "           -0.7049, -1.4439],\n",
       "          [ 4.4946, -0.4864, -1.3231,  1.8384,  0.9670, -3.0517, -1.8221,\n",
       "           -0.5035, -1.0015]]], device='cuda:0')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import model_utils\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "from model_utils import NamedEntityPredictor\n",
    "\n",
    "ner_test = NamedEntityPredictor(model_base, tokenizer, id2label_conll)\n",
    "ner_test.predict(batched_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f334839f-37bb-49fb-8860-d6cbf68d10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, get_scheduler\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_ # выполняет обрезку градиентов для избежания их взрыва при обучении\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import importlib\n",
    "import data_prep\n",
    "import model_utils\n",
    "importlib.reload(data_prep)\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "from model_utils import train_eval_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74e90e-a5ad-4aa5-ac85-db35033bc11c",
   "metadata": {},
   "source": [
    "Файн-тюн сразу всего BERTа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b19aa5-5d06-4399-95ba-25a83ac7bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:03<00:00, 62.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:02<00:00, 40.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:14<00:00, 14.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.10427977619561511 \t Test: 0.33142581998290105\n",
      "F1: 0.3707802988378528\n",
      "LR: [5.546875e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:14<00:00, 14.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.09019475296454531 \t Test: 0.32817376322216457\n",
      "F1: 0.3867521367521367\n",
      "LR: [0.0]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.88 (+0.13)  0.91 (+0.07)  0.89 (+0.10)  0.55 (+0.10)   \n",
      "MISC          0.64 (+0.06)  0.66 (-0.04)  0.65 (+0.02)  0.24 (+0.07)   \n",
      "ORG           0.84 (+0.13)  0.87 (+0.05)  0.86 (+0.10)  0.32 (+0.16)   \n",
      "PER           0.79 (+0.43)  0.92 (+0.34)  0.85 (+0.40)  0.65 (+0.35)   \n",
      "micro avg     0.81 (+0.23)  0.87 (+0.13)  0.84 (+0.19)  0.46 (+0.22)   \n",
      "macro avg     0.79 (+0.19)  0.84 (+0.10)  0.81 (+0.15)  0.44 (+0.17)   \n",
      "weighted avg  0.81 (+0.20)  0.87 (+0.13)  0.84 (+0.18)  0.46 (+0.20)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.47 (-0.04)  0.51 (+0.03)  \n",
      "MISC          0.17 (-0.05)  0.20 (+0.01)  \n",
      "ORG           0.26 (-0.04)  0.28 (+0.07)  \n",
      "PER           0.43 (+0.13)  0.52 (+0.22)  \n",
      "micro avg     0.34 (+0.03)  0.39 (+0.12)  \n",
      "macro avg     0.33 (+0.00)  0.38 (+0.09)  \n",
      "weighted avg  0.34 (+0.03)  0.39 (+0.11)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.39 (+0.11)\n",
      "pr     |    0.46 (+0.21)\n",
      "rec    |    0.34 (+0.03)\n",
      "acc    |    0.94 (+0.01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3c78a-976d-4aad-ba87-32e6a1b76e3a",
   "metadata": {},
   "source": [
    "Если файн-тюнить только классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61698a16-0b9d-480f-8d08-369911367092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:03<00:00, 61.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:02<00:00, 39.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.15063428537736476 \t Test: 0.29344349786823176\n",
      "F1: 0.33636363636363636\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 54.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.14041701169081136 \t Test: 0.28735913104021255\n",
      "F1: 0.34074074074074073\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 54.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.13575556217539758 \t Test: 0.2858989183779484\n",
      "F1: 0.3457627118644068\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.1310549315694772 \t Test: 0.28805698792415635\n",
      "F1: 0.34622042700519334\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.1281339105813296 \t Test: 0.2877367452522855\n",
      "F1: 0.3474770642201835\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.12607558481745038 \t Test: 0.2907060963090187\n",
      "F1: 0.3488773747841105\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.12383708867633567 \t Test: 0.2905823083387481\n",
      "F1: 0.34802518603319976\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.12209231433559192 \t Test: 0.2898045534980886\n",
      "F1: 0.35334476843910806\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.12107299114774231 \t Test: 0.2939505698964184\n",
      "F1: 0.3502304147465437\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.11958555796756432 \t Test: 0.2915529474891998\n",
      "F1: 0.35079726651480636\n",
      "LR: [0.0005]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.81 (+0.06)  0.87 (+0.03)  0.84 (+0.05)  0.57 (+0.12)   \n",
      "MISC          0.15 (-0.43)  0.01 (-0.69)  0.01 (-0.62)  0.05 (-0.12)   \n",
      "ORG           0.67 (-0.04)  0.73 (-0.09)  0.70 (-0.06)  0.29 (+0.13)   \n",
      "PER           0.68 (+0.32)  0.82 (+0.24)  0.74 (+0.29)  0.60 (+0.30)   \n",
      "micro avg     0.72 (+0.14)  0.71 (-0.03)  0.71 (+0.06)  0.45 (+0.21)   \n",
      "macro avg     0.58 (-0.02)  0.61 (-0.13)  0.57 (-0.09)  0.38 (+0.11)   \n",
      "weighted avg  0.65 (+0.04)  0.71 (-0.03)  0.67 (+0.01)  0.40 (+0.14)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.47 (-0.04)  0.52 (+0.04)  \n",
      "MISC          0.01 (-0.21)  0.02 (-0.17)  \n",
      "ORG           0.23 (-0.07)  0.26 (+0.05)  \n",
      "PER           0.42 (+0.12)  0.50 (+0.20)  \n",
      "micro avg     0.29 (-0.02)  0.35 (+0.08)  \n",
      "macro avg     0.28 (-0.05)  0.32 (+0.03)  \n",
      "weighted avg  0.29 (-0.02)  0.33 (+0.05)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.35 (+0.08)\n",
      "pr     |    0.45 (+0.21)\n",
      "rec    |    0.29 (-0.02)\n",
      "acc    |    0.94 (+0.01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=5e-4, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name=\"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"constant\",\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e6883-67d7-4506-b27c-1b76cc3f0760",
   "metadata": {},
   "source": [
    "##### 3.1.2  Resampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2971-6da2-498d-bc85-2ef08a22cbfa",
   "metadata": {},
   "source": [
    "Отберем топ 25% примеров из conll по схожести с wnut с помощью бинарного классификатора, затем сделаем fine-tuning модели на этих топ 25 % примерах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aefb5720-a2a9-4139-bd0d-2318ced796b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "columns = [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\", \"domain_label\"]\n",
    "\n",
    "conll_labeled = conll[\"train\"].map(lambda x: {\"domain_label\": int(0)})\n",
    "columns_to_remove = [col for col in conll_labeled.column_names if col not in columns]\n",
    "conll_labeled = conll_labeled.remove_columns(columns_to_remove)\n",
    "\n",
    "wnut_labeled = wnut[\"train\"].map(lambda x: {\"domain_label\": int(1)})\n",
    "columns_to_remove = [col for col in wnut_labeled.column_names if col not in columns]\n",
    "wnut_labeled = wnut_labeled.remove_columns(columns_to_remove)\n",
    "\n",
    "combined = concatenate_datasets([conll_labeled, wnut_labeled]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aec93414-e863-4b92-9fb2-6805ab74d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Extracting embeddings: 100%|██████████████████████████████████████████████████████████| 545/545 [00:18<00:00, 28.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# делим combined на k фолдов, учим k классификаторов на остальных k-1 фолдах, добавляем в combined wnut_score\n",
    "# для каждого примера получаем эмбеддинг предложения с помощью [CLF] токена BERTа\n",
    "# для каждого примера из тестового фолда добавляем wnut_score\n",
    "# затем из combined отбираем топ 25 % примеров по этому скору\n",
    "\n",
    "# сначала получить эмбеддинги всех предложений\n",
    "# затем сделать k классификаторов по типу leave-one-out\n",
    "# классификатор X: [batch_size, emb_len] -> wnut_scores: [batch_size]\n",
    "\n",
    "importlib.reload(model_utils)\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    combined,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_prep.PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels']),\n",
    ")\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "    embeddings = model_utils.get_sentence_embeddings(model, batch)  # [batch_size, emb_size]\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)  # [num_rows, emb_size]\n",
    "\n",
    "combined = combined.add_column(\"cls_embedding\", all_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73daa64f-1670-4a02-a2fb-67500c954ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'domain_label', 'cls_embedding'],\n",
       "    num_rows: 17435\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "119195eb-5819-4c8d-86f0-03a9248b767e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17435, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['cls_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "533747b9-ecbf-4c83-99e6-0a55ba662d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 243.48it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 238.16it/s]\n",
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 248.33it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 239.53it/s]\n",
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 247.58it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 250.15it/s]\n",
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 252.17it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 249.82it/s]\n",
      "Train embeddings: 100%|██████████████████████████████████████████████████████████████| 436/436 [00:01<00:00, 247.13it/s]\n",
      "Test embeddings: 100%|███████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 239.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# теперь добавим wnut_score в combined для дальнейшего отбора\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "wnut_scores = np.zeros(len(combined))\n",
    "\n",
    "indices = np.arange(len(combined))\n",
    "for train_idxs, test_idxs in kf.split(indices):\n",
    "    train_examples = combined.select(train_idxs)\n",
    "    test_examples = combined.select(test_idxs)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_examples, batch_size=32,\n",
    "        collate_fn=data_prep.PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_examples, batch_size=32,\n",
    "        collate_fn=data_prep.PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "    )\n",
    "\n",
    "    X_train, Y_train = [], []\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=\"Train embeddings\"):\n",
    "        embs = torch.stack(batch[\"cls_embedding\"]).cpu().numpy()\n",
    "        domains = torch.stack(batch['domain_label']).cpu().numpy()\n",
    "        X_train.append(embs)\n",
    "        Y_train.append(domains)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    Y_train = np.concatenate(Y_train)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    X_test = []\n",
    "    for batch in tqdm(test_dataloader, desc=\"Test embeddings\"):\n",
    "        embs = torch.stack(batch[\"cls_embedding\"]).cpu().numpy()\n",
    "        X_test.append(embs)\n",
    "    X_test = np.concatenate(X_test)\n",
    "    probas = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    wnut_scores[test_idxs] = probas\n",
    "\n",
    "combined = combined.add_column(\"wnut_score\", wnut_scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "270e629e-0d7e-4948-96d7-1c7ba129dc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'domain_label', 'cls_embedding', 'wnut_score'],\n",
       "    num_rows: 17435\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ac8b17f-8ba2-465e-955d-0a4bc5071d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fbf4c2815b4054a12d826b7d30cb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# отбираем топ 25 % объектов из conll по схожести с датасетом с wnut\n",
    "\n",
    "conll_subset = combined.filter(lambda x: x[\"domain_label\"] == 0)\n",
    "\n",
    "conll_sorted = conll_subset.sort(\"wnut_score\", reverse=True) \n",
    "\n",
    "# построить гистограмму с wnut_score\n",
    "\n",
    "top_25_count = int(0.25 * len(conll_sorted))\n",
    "top_25_conll = conll_sorted.select(range(top_25_count))\n",
    "\n",
    "top_25_conll = top_25_conll.remove_columns([\n",
    "    col for col in top_25_conll.column_names if col not in ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37fe00-bad9-42b6-bc15-27b870eeec2a",
   "metadata": {},
   "source": [
    "Теперь есть топ 25 % примеров из CONLL для файн-тюна BERTа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74c067dd-8ae1-4fa6-8b83-1909bae77d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3510\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_25_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b4409ed-fcc5-4f65-94ae-c560cb05fe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'text_labels'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45d087-9e87-44bd-a23c-14636f49dfa0",
   "metadata": {},
   "source": [
    "Размеры датасетов wnut['train'] и top_25_conll почти одинаковы. Посмотрим что будет, если файн-тюнить классификатор на top_25_conll с теми же гиперпараметрами, что и для файн-тюна на wnut['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a48601a-74b6-4228-8295-59d1608df8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:03<00:00, 62.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:02<00:00, 40.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.03518075127503835 \t Test: 0.3934418582989846\n",
      "F1: 0.3237721021611002\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.03055757205293048 \t Test: 0.397770101296497\n",
      "F1: 0.32731554160125587\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.028332651730901985 \t Test: 0.40892010152247954\n",
      "F1: 0.33399131464666404\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.02647041574501517 \t Test: 0.40972930735644\n",
      "F1: 0.3285939968404423\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.02486557644283907 \t Test: 0.40815789307709094\n",
      "F1: 0.332934131736527\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.024267765940352216 \t Test: 0.41601530959208805\n",
      "F1: 0.33147410358565743\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.023060493065382947 \t Test: 0.4129573016163008\n",
      "F1: 0.3297576479936432\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.02209912260084159 \t Test: 0.41661112456593985\n",
      "F1: 0.33042789223454827\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 61.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.021720438595564866 \t Test: 0.4194184423964701\n",
      "F1: 0.3280632411067194\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 220/220 [00:03<00:00, 60.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.020791363586216572 \t Test: 0.41656336183717224\n",
      "F1: 0.32986389111289033\n",
      "LR: [0.0005]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.89 (+0.14)  0.92 (+0.08)  0.90 (+0.11)  0.40 (-0.05)   \n",
      "MISC          0.64 (+0.06)  0.75 (+0.05)  0.69 (+0.06)  0.13 (-0.04)   \n",
      "ORG           0.83 (+0.12)  0.90 (+0.08)  0.86 (+0.10)  0.15 (-0.01)   \n",
      "PER           0.83 (+0.47)  0.93 (+0.35)  0.88 (+0.43)  0.61 (+0.31)   \n",
      "micro avg     0.82 (+0.24)  0.89 (+0.15)  0.86 (+0.21)  0.29 (+0.05)   \n",
      "macro avg     0.80 (+0.20)  0.87 (+0.13)  0.83 (+0.17)  0.32 (+0.05)   \n",
      "weighted avg  0.82 (+0.21)  0.89 (+0.15)  0.86 (+0.20)  0.36 (+0.10)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.49 (-0.02)  0.44 (-0.04)  \n",
      "MISC          0.20 (-0.02)  0.16 (-0.03)  \n",
      "ORG           0.32 (+0.02)  0.21 (+0.00)  \n",
      "PER           0.49 (+0.19)  0.54 (+0.24)  \n",
      "micro avg     0.38 (+0.07)  0.33 (+0.06)  \n",
      "macro avg     0.38 (+0.05)  0.34 (+0.05)  \n",
      "weighted avg  0.38 (+0.07)  0.36 (+0.08)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.33 (+0.06)\n",
      "pr     |    0.29 (+0.05)\n",
      "rec    |    0.38 (+0.07)\n",
      "acc    |    0.93 (-0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=5e-4, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = top_25_conll\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"constant\",\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765622d-8567-414d-b803-e557c8f19e2c",
   "metadata": {},
   "source": [
    "#### 3.2 Unsupervised DA\n",
    "\n",
    "##### 3.2.1 Proxy-labels methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c82b3-2ca9-422a-a661-8e2acde61985",
   "metadata": {},
   "source": [
    "До этого использовались размеченные данные. Теперь будем отбирать наиболее уверенные с точки зрения пре-тренированной модели примеры, делать предсказания и дообучать на этих прокси-лейблах, чтобы перенести знания из целевого домена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cae3690a-81f1-43af-8e08-f71084c827ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# лучше сделать рестарт ядра перед запуском ячеек ниже\n",
    "# ниже повторная предобработка данных\n",
    "import datasets\n",
    "import typing as tp\n",
    "import inspect\n",
    "\n",
    "conll = datasets.load_dataset(\"conll2003\") # DatasetDict\n",
    "wnut = datasets.load_dataset(\"wnut_17\")\n",
    "\n",
    "CONLL_NER_TAGS = conll['train'].features['ner_tags'].feature.names # всего 9 тегов\n",
    "WNUT_NER_TAGS = wnut['train'].features['ner_tags'].feature.names\n",
    "\n",
    "label_mapping = {\n",
    "    'O': 'O',\n",
    "    'B-location': 'B-LOC',\n",
    "    'I-location': 'I-LOC',\n",
    "    'B-group': 'B-ORG',\n",
    "    'B-corporation': 'B-ORG',\n",
    "    'B-person': 'B-PER',\n",
    "    'B-creative-work': 'B-MISC',\n",
    "    'B-product': 'B-MISC',\n",
    "    'I-person': 'I-PER',\n",
    "    'I-creative-work': 'I-MISC',\n",
    "    'I-corporation': 'I-ORG',\n",
    "    'I-group': 'I-ORG',\n",
    "    'I-product': 'I-MISC'\n",
    "}\n",
    "\n",
    "label2id_conll= {'O': 0,\n",
    " 'B-PER': 1,\n",
    " 'I-PER': 2,\n",
    " 'B-ORG': 3,\n",
    " 'I-ORG': 4,\n",
    " 'B-LOC': 5,\n",
    " 'I-LOC': 6,\n",
    " 'B-MISC': 7,\n",
    " 'I-MISC': 8}\n",
    "\n",
    "labelindexmapping = {WNUT_NER_TAGS.index(k):CONLL_NER_TAGS.index(v) for k, v in label_mapping.items()}\n",
    "\n",
    "import data_prep\n",
    "\n",
    "converted_wnut = wnut.map(lambda x: data_prep.convert_label_sequence(x, labelindexmapping))\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (pipeline, \n",
    "        AutoModelForTokenClassification, AutoTokenizer, \n",
    "        BertForTokenClassification, BertTokenizer)\n",
    "\n",
    "import importlib\n",
    "import data_prep\n",
    "\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "\n",
    "assert tokenizer.decode(test_result['input_ids']) == '[CLS] His name is Jerry Abrahamson [SEP]'\n",
    "\n",
    "                                     #CLS     His  name is    Jerry    Abraham   ##son      SEP\n",
    "assert test_result['text_labels'] == ['O'] + [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\",  \"I-PER\"] + [\"O\"]\n",
    "\n",
    "conll = conll.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "wnut = converted_wnut\n",
    "wnut = wnut.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "conll.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "wnut.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "\n",
    "input_keys = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "batch_size = 32\n",
    "wnut_train_dataloader = torch.utils.data.DataLoader(wnut[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "wnut_test_dataloader = torch.utils.data.DataLoader(wnut[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_train_dataloader = torch.utils.data.DataLoader(conll[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_test_dataloader = torch.utils.data.DataLoader(conll[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3669b59-c2ac-458c-a41b-a64fd8ffb249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "420d3ff0-5af4-4735-b175-cb4ce676beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 47, 9])\n",
      "[0.9770016074180603, 0.9972400665283203, 0.9488641023635864, 0.9996575713157654, 0.9515469074249268, 0.9970041513442993, 0.999862551689148, 0.999904215335846, 0.9905007481575012, 0.9993622303009033, 0.989292562007904, 0.9799342751502991, 0.9869406819343567, 0.9998921751976013, 0.9998874068260193, 0.9990612864494324, 0.957036018371582, 0.9998361468315125, 0.9946780204772949, 0.9996030926704407, 0.9914761185646057, 0.9993963241577148, 0.9936321377754211, 0.999690592288971, 0.999440610408783, 0.9997891187667847, 0.9997739791870117, 0.9849512577056885, 0.9998698234558105, 0.9916747808456421, 0.9861404895782471, 0.9998873472213745]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(wnut_train_dataloader))\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                 token_type_ids=batch[\"token_type_ids\"],\n",
    "                 attention_mask=batch[\"attention_mask\"],\n",
    "                 labels=batch[\"labels\"], return_dict=True)\n",
    "\n",
    "print(outputs.logits.shape)\n",
    "attn_mask = batch['attention_mask']\n",
    "probs = torch.softmax(outputs.logits, dim=-1)\n",
    "max_probs = probs.max(dim=-1).values\n",
    "\n",
    "confidences = []\n",
    "for i in range(max_probs.shape[0]):\n",
    "    mask = attn_mask[i].bool()\n",
    "    conf = max_probs[i][mask].mean().item()\n",
    "    confidences.append(conf)\n",
    "\n",
    "print(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1a293fd-5345-46dc-b0f5-3bd189ac7716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7b1de5046a4b2a88a138be8320c9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_confidences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(wnut_train_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)             # [B, T, C]\n",
    "        max_probs = probs.max(dim=-1).values                      # [B, T]\n",
    "\n",
    "        for i in range(input_ids.size(0)):\n",
    "            real_token_mask = attention_mask[i].bool()            # [T]\n",
    "            conf = max_probs[i][real_token_mask].mean().item()\n",
    "            all_confidences.append(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33ba3553-3bf1-4acb-9fee-2b6d25992299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# убедимся, что длина совпадает\n",
    "assert len(all_confidences) == len(wnut[\"train\"])\n",
    "\n",
    "# добавим как новое поле\n",
    "wnut[\"train\"] = wnut[\"train\"].add_column(\"confidence\", all_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96c865f6-a568-4406-9a99-3581074dbaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a33e24cf99472fb4cb8e94beadff03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confidences = np.array(all_confidences)\n",
    "threshold = np.percentile(confidences, 50)\n",
    "\n",
    "filtered_wnut = wnut[\"train\"].filter(lambda example: example[\"confidence\"] >= threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a57e62-1e66-4c70-ab08-6c34c25d165a",
   "metadata": {},
   "source": [
    "Датасет для файн-тюна готов.\n",
    "\n",
    "Как обычно загрузим базовую модель (без файн-тюна) и будем сравнивать результаты с ней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a76712a7-1f14-4022-8c05-239336540b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'text_labels', 'confidence'],\n",
       "    num_rows: 1697\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db43eff6-6e25-421d-a3d4-332fdd7cdd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c80ed968aae4c3e9c894c9f3554ed71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# добавляем прокси-метки в датасет\n",
    "filtered_wnut_dataloader = torch.utils.data.DataLoader(filtered_wnut,        \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "\n",
    "# снова получим прокси-лейблы для топ 50 процентов самых уверенных примеров из wnut_train\n",
    "# получаем прокси-лейблы\n",
    "ner = NamedEntityPredictor(model_ft, tokenizer, id2label=id2label_conll)\n",
    "text_proxy_labels = []\n",
    "proxy_labels = []\n",
    "\n",
    "for batch in filtered_wnut_dataloader:\n",
    "    text_labels_batch = ner.predict(batch)['predicted_labels']  # list[list[str]]\n",
    "    text_proxy_labels.extend(text_labels_batch)\n",
    "\n",
    "    # переводим текстовые метки в индексы\n",
    "    index_batch = [\n",
    "        [label2id_conll[label] for label in seq]\n",
    "        for seq in text_labels_batch\n",
    "    ]\n",
    "    proxy_labels.extend(index_batch)\n",
    "\n",
    "# создаем новый датасет с заменой labels и text_labels\n",
    "filtered_wnut_proxy = filtered_wnut.remove_columns(['labels', 'text_labels']) \\\n",
    "                                   .add_column('labels', proxy_labels) \\\n",
    "                                   .add_column('text_labels', text_proxy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17cae7db-2947-4f4f-8530-5bb69734c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:03<00:00, 60.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:02<00:00, 39.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 58.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.007117963811082365 \t Test: 0.44186677158246807\n",
      "F1: 0.2869022869022869\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 59.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.005920233396175709 \t Test: 0.4306153497302238\n",
      "F1: 0.2958927074601844\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 59.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.005193093086040153 \t Test: 0.4277489717911791\n",
      "F1: 0.3113924050632912\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 59.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.004749315939887175 \t Test: 0.42918289875910604\n",
      "F1: 0.33020477815699656\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 59.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.004427257754184985 \t Test: 0.43323168705826925\n",
      "F1: 0.3350383631713555\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 59.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.004228786813969959 \t Test: 0.4409709957278805\n",
      "F1: 0.33801617709663684\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 58.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.00400009269175494 \t Test: 0.44045585005279675\n",
      "F1: 0.33603411513859277\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 59.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.003793370290104491 \t Test: 0.44196327267513597\n",
      "F1: 0.33773987206823025\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 58.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.0036362559571601104 \t Test: 0.44582081190597866\n",
      "F1: 0.341025641025641\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 59.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.003502310390222957 \t Test: 0.4455311949146383\n",
      "F1: 0.3388676032354193\n",
      "LR: [0.0001]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.78 (+0.03)  0.85 (+0.01)  0.82 (+0.03)  0.50 (+0.05)   \n",
      "MISC          0.60 (+0.02)  0.72 (+0.02)  0.65 (+0.02)  0.16 (-0.01)   \n",
      "ORG           0.75 (+0.04)  0.85 (+0.03)  0.80 (+0.04)  0.17 (+0.01)   \n",
      "PER           0.62 (+0.26)  0.82 (+0.24)  0.71 (+0.26)  0.55 (+0.25)   \n",
      "micro avg     0.70 (+0.12)  0.82 (+0.08)  0.76 (+0.11)  0.31 (+0.07)   \n",
      "macro avg     0.69 (+0.09)  0.81 (+0.07)  0.74 (+0.08)  0.34 (+0.07)   \n",
      "weighted avg  0.71 (+0.10)  0.82 (+0.08)  0.76 (+0.10)  0.36 (+0.10)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.52 (+0.01)  0.51 (+0.03)  \n",
      "MISC          0.19 (-0.03)  0.17 (-0.02)  \n",
      "ORG           0.32 (+0.02)  0.22 (+0.01)  \n",
      "PER           0.46 (+0.16)  0.50 (+0.20)  \n",
      "micro avg     0.37 (+0.06)  0.34 (+0.07)  \n",
      "macro avg     0.37 (+0.04)  0.35 (+0.06)  \n",
      "weighted avg  0.37 (+0.06)  0.36 (+0.08)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.34 (+0.07)\n",
      "pr     |    0.31 (+0.07)\n",
      "rec    |    0.37 (+0.06)\n",
      "acc    |    0.93 (+0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, param in model_ft.named_parameters():\n",
    "#     if name.startswith(\"bert.embeddings\") or name.startswith(\"bert.encoder.layer.0\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.1\") or name.startswith(\"bert.encoder.layer.2\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.3\") or name.startswith(\"bert.encoder.layer.4\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.5\") or name.startswith(\"bert.encoder.layer.6\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.7\"):\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=1e-4, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = filtered_wnut_proxy\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"constant\",\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94ee7b-2b13-4848-9e07-7f1c0c7a3fa6",
   "metadata": {},
   "source": [
    "Если заморозить первые 8 слоев BERTа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cb9d262-1d15-4fac-8d45-293f75bff83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:03<00:00, 61.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:02<00:00, 39.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.00406544765285608 \t Test: 0.44069803248585004\n",
      "F1: 0.31780366056572384\n",
      "LR: [2e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.0024271077056476306 \t Test: 0.44126595496947385\n",
      "F1: 0.3608957795004307\n",
      "LR: [1.7777777777777777e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.0018782729599717352 \t Test: 0.46295647640471105\n",
      "F1: 0.35946297098310964\n",
      "LR: [1.555555555555556e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.0014893334119163984 \t Test: 0.46688862181740043\n",
      "F1: 0.36449806118052563\n",
      "LR: [1.3333333333333333e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.0010908352342764978 \t Test: 0.4683896000554532\n",
      "F1: 0.368006993006993\n",
      "LR: [1.1111111111111113e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.0009474135420821546 \t Test: 0.4769150627616011\n",
      "F1: 0.37037037037037035\n",
      "LR: [8.888888888888888e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.0008474013662193118 \t Test: 0.47173116921826647\n",
      "F1: 0.37001315212626046\n",
      "LR: [6.666666666666667e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.0007515545706232035 \t Test: 0.4748352336938734\n",
      "F1: 0.3732394366197183\n",
      "LR: [4.444444444444444e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.0006943032658965192 \t Test: 0.4771714671112505\n",
      "F1: 0.3720316622691293\n",
      "LR: [2.222222222222222e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 37.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.0006872540363695399 \t Test: 0.4766671623068827\n",
      "F1: 0.3718285214348206\n",
      "LR: [0.0]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.81 (+0.06)  0.87 (+0.03)  0.84 (+0.05)  0.52 (+0.07)   \n",
      "MISC          0.61 (+0.03)  0.72 (+0.02)  0.66 (+0.03)  0.19 (+0.02)   \n",
      "ORG           0.81 (+0.10)  0.88 (+0.06)  0.84 (+0.08)  0.19 (+0.03)   \n",
      "PER           0.74 (+0.38)  0.89 (+0.31)  0.81 (+0.36)  0.60 (+0.30)   \n",
      "micro avg     0.76 (+0.18)  0.86 (+0.12)  0.81 (+0.16)  0.35 (+0.11)   \n",
      "macro avg     0.74 (+0.14)  0.84 (+0.10)  0.79 (+0.13)  0.38 (+0.11)   \n",
      "weighted avg  0.76 (+0.15)  0.86 (+0.12)  0.81 (+0.15)  0.40 (+0.14)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.53 (+0.02)  0.52 (+0.04)  \n",
      "MISC          0.22 (+0.00)  0.20 (+0.01)  \n",
      "ORG           0.33 (+0.03)  0.25 (+0.04)  \n",
      "PER           0.49 (+0.19)  0.54 (+0.24)  \n",
      "micro avg     0.39 (+0.08)  0.37 (+0.10)  \n",
      "macro avg     0.39 (+0.06)  0.38 (+0.09)  \n",
      "weighted avg  0.39 (+0.08)  0.39 (+0.11)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.37 (+0.10)\n",
      "pr     |    0.35 (+0.11)\n",
      "rec    |    0.39 (+0.09)\n",
      "acc    |    0.93 (+0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for param in model_ft.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "for name, param in model_ft.named_parameters():\n",
    "    if name.startswith(\"bert.embeddings\") or name.startswith(\"bert.encoder.layer.0\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.1\") or name.startswith(\"bert.encoder.layer.2\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.3\") or name.startswith(\"bert.encoder.layer.4\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.5\") or name.startswith(\"bert.encoder.layer.6\") or \\\n",
    "       name.startswith(\"bert.encoder.layer.7\"):\n",
    "        param.requires_grad = False\n",
    "    \n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = filtered_wnut_proxy\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccba18b6-c15b-4f6c-b2a3-7b4a2f6be3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    0,    0,    0,    0,    0,    0,    3,    4,    4,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100], device='cuda:0')\n",
      "tensor([   0,    0,    3,    4,    4,    4,    0,    3,    4,    4,    0,    0,\n",
      "           0,    7,    8,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
      "        -100], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# сравним настоящие и прокси метки на одном примере\n",
    "filtered_wnut_dataloader_proxy = torch.utils.data.DataLoader(filtered_wnut_proxy,        \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "\n",
    "print(next(iter(filtered_wnut_dataloader))['labels'][0])\n",
    "print(next(iter(filtered_wnut_dataloader_proxy))['labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a63fb-c6fa-4b85-aaf3-1bd5ac9e865f",
   "metadata": {},
   "source": [
    "Сравнение с обычным файн-тюном на всем трейне wnut при тех же гиперпараметрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0b8e66c-072c-435f-a166-a2172a416ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:03<00:00, 61.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:02<00:00, 39.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 52.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.22862686792162257 \t Test: 0.361539889310981\n",
      "F1: 0.29442970822281167\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 54.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.18956494040647956 \t Test: 0.3072130558897316\n",
      "F1: 0.3168124392614189\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 54.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.174231559421136 \t Test: 0.2935180494814743\n",
      "F1: 0.3348982785602504\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 54.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.16729643700696598 \t Test: 0.289543953390769\n",
      "F1: 0.33604336043360433\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 54.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.1631018113971592 \t Test: 0.2885349765879872\n",
      "F1: 0.3410596026490066\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 54.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.15982489007586073 \t Test: 0.2855894353102755\n",
      "F1: 0.3401662049861496\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 53.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.1573744175137482 \t Test: 0.2835231116156519\n",
      "F1: 0.34073251942286353\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 53.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.15525568419388522 \t Test: 0.28294920548796654\n",
      "F1: 0.3437152391546162\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 53.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.15351878430381 \t Test: 0.2822234291169379\n",
      "F1: 0.3479224376731302\n",
      "LR: [0.0001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:01<00:00, 53.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.1517657469261751 \t Test: 0.2816503979725602\n",
      "F1: 0.34883720930232553\n",
      "LR: [0.0001]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.81 (+0.06)  0.88 (+0.04)  0.84 (+0.05)  0.55 (+0.10)   \n",
      "MISC          0.62 (+0.04)  0.64 (-0.06)  0.63 (+0.00)  0.18 (+0.01)   \n",
      "ORG           0.71 (+0.00)  0.75 (-0.07)  0.73 (-0.03)  0.35 (+0.19)   \n",
      "PER           0.62 (+0.26)  0.78 (+0.20)  0.69 (+0.24)  0.56 (+0.26)   \n",
      "micro avg     0.70 (+0.12)  0.78 (+0.04)  0.74 (+0.09)  0.43 (+0.19)   \n",
      "macro avg     0.69 (+0.09)  0.76 (+0.02)  0.73 (+0.07)  0.41 (+0.14)   \n",
      "weighted avg  0.70 (+0.09)  0.78 (+0.04)  0.74 (+0.08)  0.42 (+0.16)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.49 (-0.02)  0.52 (+0.04)  \n",
      "MISC          0.11 (-0.11)  0.14 (-0.05)  \n",
      "ORG           0.24 (-0.06)  0.28 (+0.07)  \n",
      "PER           0.37 (+0.07)  0.44 (+0.14)  \n",
      "micro avg     0.29 (-0.02)  0.35 (+0.08)  \n",
      "macro avg     0.30 (-0.03)  0.35 (+0.06)  \n",
      "weighted avg  0.29 (-0.02)  0.34 (+0.06)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.35 (+0.08)\n",
      "pr     |    0.43 (+0.19)\n",
      "rec    |    0.29 (-0.02)\n",
      "acc    |    0.94 (+0.01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# берем не топ 50 процентов по уверенности модели, а просто 50 процентов рандомных примеров из wnut train\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random_indices = random.sample(range(len(wnut[\"train\"])), len(filtered_wnut))\n",
    "wnut_train_random_subs = [wnut[\"train\"][i] for i in random_indices]\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for param in model_ft.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, param in model_ft.named_parameters():\n",
    "#     if name.startswith(\"bert.embeddings\") or name.startswith(\"bert.encoder.layer.0\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.1\") or name.startswith(\"bert.encoder.layer.2\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.3\") or name.startswith(\"bert.encoder.layer.4\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.5\") or name.startswith(\"bert.encoder.layer.6\") or \\\n",
    "#        name.startswith(\"bert.encoder.layer.7\"):\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=1e-4, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut_train_random_subs\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"constant\",\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee54d19-bd7a-408d-9c20-6e3641b2c689",
   "metadata": {},
   "source": [
    "##### 3.2.2 Unsupervised pretraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1d9c77-9baf-40df-bd9d-b69435ed5a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Dataset' has no attribute 'from_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 9\u001b[0m reddit_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m(data)\n\u001b[1;32m     10\u001b[0m reddit_dataset\u001b[38;5;241m=\u001b[39m reddit_dataset\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'from_list'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('reddit_sample.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "from datasets import Dataset\n",
    "reddit_dataset = Dataset.from_list(data)\n",
    "reddit_dataset= reddit_dataset.rename_column(\"words\", \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef721adc-48a7-491b-a720-51f4a9057f25",
   "metadata": {},
   "source": [
    "Аналогично 3.1.2 отберем 100 000 примеров, наиболее похожих на wnut['train'] и дообучим BERT на задачу masked LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41d250-a6ae-487f-aef4-c0224290d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bd22f643-6c2c-4fea-9c9b-4b2a052e85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['tokens'] # называется токены, но в датасете это просто списки слов\n",
    "columns_to_remove = [col for col in wnut['train'].column_names if col not in columns_to_keep]\n",
    "\n",
    "wnut_train_new = wnut['train'].remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "93bee0c6-9791-4758-9e50-ab7e19fa0bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eb07392f-b26d-4f70-9fa6-31c4fcec84aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens'],\n",
       "    num_rows: 500000\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a0764dbd-46c1-4d1c-bfd2-b73cbd8d271d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5af39b22ea48c8b06be261500e47a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "reddit_dataset = reddit_dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "114b96e3-3356-45a9-bf05-9315e6c6ab12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8595bd3340834fbbb270e65de581adc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wnut_train_new = wnut_train_new.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cd5f7dfd-d235-4e08-bda7-b97ef0c815e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94405fb7404047a982519b3b43596394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1620da5f264d411aa23d68f8c3b56107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reddit_dataset = reddit_dataset.map(lambda x: {\"domain_label\": int(0)})\n",
    "wnut_train_new = wnut_train_new.map(lambda x: {\"domain_label\": int(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "567a5cde-f909-4e18-ac34-4f8ecf7b3ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'domain_label'],\n",
       "    num_rows: 500000\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6977bb96-e935-46aa-b058-50fa932c32c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'domain_label'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "50a5e4b0-ce9e-49c2-97ea-0577d2f40d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined = concatenate_datasets([reddit_dataset, wnut_train_new]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d788bb48-ec32-4ad3-88d0-233aabe0e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4e968566-3394-4b2c-ab74-3f4fb3501cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['domain_label'][500_001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "676900ce-c229-495e-a350-36f206f3e707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ff34e614-2909-4b9b-9515-018012145c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ex in enumerate(combined):\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if ex.get(key) is None:\n",
    "            print(f\"Пропущено значение в примере {i}: {key} = None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2bbb822b-ee51-4572-a5cf-3b12f3ab2240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4523fb5c06374bf096e1cec16ee138c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting embeddings:   0%|          | 0/15732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "import data_prep\n",
    "from data_prep import PadSequence\n",
    "\n",
    "collate_fn = PadSequence(['input_ids','token_type_ids','attention_mask'])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    combined,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "batch\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "    batch = {k: batch[k].to(device) for k in ['input_ids','token_type_ids','attention_mask']}\n",
    "    with torch.no_grad():\n",
    "        outputs=model(**batch)\n",
    "        cls_emb = model(**batch).last_hidden_state[:, 0] # (batch_size, emb_size=768)\n",
    "    all_embeddings.append(cls_emb.cpu())\n",
    "\n",
    "all_embeddings = torch.cat(all_embeddings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c2aa28c8-f191-4b0b-98dc-c2f08220c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.add_column(\"cls_embedding\", all_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "078f902a-31c0-44df-ac52-8ee2df3654e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'domain_label', 'cls_embedding'],\n",
       "    num_rows: 503394\n",
       "})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eb62adb6-0cf8-4477-ba0e-281f510965c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44252914f7f34612bacc358647b4f2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training classifiers:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "k = 5\n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "wnut_scores = np.zeros(len(combined))\n",
    "X = combined[\"cls_embedding\"].cpu().numpy()\n",
    "Y = combined[\"domain_label\"].cpu().numpy()\n",
    "\n",
    "indices = np.arange(len(combined))\n",
    "for i, (train_idxs, test_idxs) in enumerate(tqdm(kf.split(X, Y), total=k, desc=\"Training classifiers\")):\n",
    "    X_train, X_test = X[train_idxs], X[test_idxs]\n",
    "    Y_train = Y[train_idxs]\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "    clf.fit(X_train, Y_train)\n",
    "    probas = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    wnut_scores[test_idxs] = probas\n",
    "    \n",
    "combined = combined.add_column(\"wnut_score\", wnut_scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e0ee0501-4baa-4f97-8e93-bbaa8c517178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e26c4c63be4e08b7643686f77677cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/503394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# отбираем топ 20 % объектов из reddit_dataset по схожести с датасетом с wnut\n",
    "\n",
    "reddit_subset = combined.filter(lambda x: x[\"domain_label\"] == 0)\n",
    "\n",
    "reddit_sorted = reddit_subset.sort(\"wnut_score\", reverse=True) \n",
    "\n",
    "# построить гистограмму с wnut_score\n",
    "\n",
    "top_20_count = int(0.20 * len(reddit_sorted))\n",
    "top_20_reddit = reddit_sorted.select(range(top_20_count))\n",
    "\n",
    "# top_20_reddit = top_20_reddit.remove_columns([\n",
    "#     col for col in top_20_reddit.column_names if col not in ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a8f2909e-5d50-4dfd-906b-3a45161af478",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_examples = top_20_reddit['tokens']\n",
    "\n",
    "import json\n",
    "with open(\"sel_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sel_examples, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb89210-2a55-4ddf-915d-d231320c2cdd",
   "metadata": {},
   "source": [
    "Теперь есть топ 20 % из 500 000 примеров для файн-тюна самого BERTа на задаче masked LM.\n",
    "\n",
    "Код для файн-тюнинга BERTа на задаче MLM лежит в ноутбуке mlm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb07c949-2efa-4cc6-834c-80dcbbe8be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at model_base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import typing as tp\n",
    "import inspect\n",
    "import data_prep\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch \n",
    "from transformers import (pipeline, \n",
    "        AutoModelForTokenClassification, AutoTokenizer, \n",
    "        BertForTokenClassification, BertTokenizer)\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW, BertTokenizer, BertForMaskedLM, set_seed, BertTokenizerFast\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import data_prep\n",
    "import importlib\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "# conll = load_dataset(\"datasets/conll2003\")\n",
    "# wnut = load_dataset(\"datasets/wnut_17\")\n",
    "\n",
    "conll = datasets.load_dataset(\"conll2003\") # DatasetDict\n",
    "wnut = datasets.load_dataset(\"wnut_17\")\n",
    "\n",
    "CONLL_NER_TAGS = conll['train'].features['ner_tags'].feature.names # всего 9 тегов\n",
    "WNUT_NER_TAGS = wnut['train'].features['ner_tags'].feature.names\n",
    "\n",
    "label_mapping = {\n",
    "    'O': 'O',\n",
    "    'B-location': 'B-LOC',\n",
    "    'I-location': 'I-LOC',\n",
    "    'B-group': 'B-ORG',\n",
    "    'B-corporation': 'B-ORG',\n",
    "    'B-person': 'B-PER',\n",
    "    'B-creative-work': 'B-MISC',\n",
    "    'B-product': 'B-MISC',\n",
    "    'I-person': 'I-PER',\n",
    "    'I-creative-work': 'I-MISC',\n",
    "    'I-corporation': 'I-ORG',\n",
    "    'I-group': 'I-ORG',\n",
    "    'I-product': 'I-MISC'\n",
    "}\n",
    "\n",
    "labelindexmapping = {WNUT_NER_TAGS.index(k):CONLL_NER_TAGS.index(v) for k, v in label_mapping.items()}\n",
    "\n",
    "converted_wnut = wnut.map(lambda x: data_prep.convert_label_sequence(x, labelindexmapping))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model_name = \"dslim/bert-base-NER\"\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"model_base\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "label2id_conll= {'O': 0,\n",
    " 'B-PER': 1,\n",
    " 'I-PER': 2,\n",
    " 'B-ORG': 3,\n",
    " 'I-ORG': 4,\n",
    " 'B-LOC': 5,\n",
    " 'I-LOC': 6,\n",
    " 'B-MISC': 7,\n",
    " 'I-MISC': 8\n",
    "}\n",
    "\n",
    "id2label_conll = {v : k for k, v in label2id_conll.items()}\n",
    "\n",
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "\n",
    "assert tokenizer.decode(test_result['input_ids']) == '[CLS] His name is Jerry Abrahamson [SEP]'\n",
    "\n",
    "                                     #CLS     His  name is    Jerry    Abraham   ##son      SEP\n",
    "assert test_result['text_labels'] == ['O'] + [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\",  \"I-PER\"] + [\"O\"]\n",
    "\n",
    "conll = conll.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "wnut = converted_wnut\n",
    "wnut = wnut.map(lambda x: data_prep.tokenize_and_preserve_tags(x, tokenizer, label2id_conll))\n",
    "\n",
    "conll.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "wnut.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "\n",
    "input_keys = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "batch_size = 16\n",
    "wnut_train_dataloader = torch.utils.data.DataLoader(wnut[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "wnut_test_dataloader = torch.utils.data.DataLoader(wnut[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_train_dataloader = torch.utils.data.DataLoader(conll[\"train\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))\n",
    "conll_test_dataloader = torch.utils.data.DataLoader(conll[\"test\"], \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    collate_fn=data_prep.PadSequence(input_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aaeab9fc-7f25-4bbc-95bf-98445cbde02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 95.293, 'PER': 1.485, 'ORG': 1.068, 'LOC': 1.125, 'MISC': 1.028}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = {k : 0 for k in label2id_conll.keys()}\n",
    "for example in wnut['train']['text_labels']:\n",
    "    for label in example:\n",
    "        counter[label] += 1\n",
    "\n",
    "total=0\n",
    "for count in counter.values():\n",
    "    total+=count\n",
    "\n",
    "freqs = {\n",
    "    \"O\" : counter['O']/total*100,\n",
    "    \"PER\" : (counter['B-PER']+counter['I-PER'])/total*100,\n",
    "    \"ORG\" : (counter['B-ORG']+counter['I-ORG'])/total*100,\n",
    "    \"LOC\" : (counter['B-LOC']+counter['I-LOC'])/total*100,\n",
    "    \"MISC\" : (counter['B-MISC']+counter['I-MISC'])/total*100  \n",
    "}\n",
    "\n",
    "freqs = {k : round(v,3) for k, v in freqs.items()}\n",
    "\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "524e79b3-5a1e-4513-a7bc-e2d053dc675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, get_scheduler, BertTokenizerFast\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_ # выполняет обрезку градиентов для избежания их взрыва при обучении\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import importlib\n",
    "import data_prep\n",
    "import model_utils\n",
    "importlib.reload(data_prep)\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "from model_utils import train_eval_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33902aff-e5bb-44f0-bae5-4941b9240cd2",
   "metadata": {},
   "source": [
    "Если использовать continued pre-trained модель, полученную в ноутбуке mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a9d5c98-394a-4caa-8c47-787a354b7352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d8269b1-c661-4500-b34f-a1a6948bb27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 216/216 [00:03<00:00, 61.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 81/81 [00:02<00:00, 39.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.18073643642552023 \t Test: 0.31233715891470143\n",
      "F1: 0.12943962115232832\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.16398033358926226 \t Test: 0.2949932648995776\n",
      "F1: 0.18311874105865525\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.15546671094748896 \t Test: 0.29035682539329116\n",
      "F1: 0.21565934065934067\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.14963685356500284 \t Test: 0.2865238900721809\n",
      "F1: 0.23489932885906037\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.14564415829107516 \t Test: 0.2863646913549782\n",
      "F1: 0.24270557029177722\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.1424156034653875 \t Test: 0.2845322620353581\n",
      "F1: 0.24481865284974094\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.13966224303232952 \t Test: 0.2838636797703343\n",
      "F1: 0.24935732647814912\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.13756745501897985 \t Test: 0.28678881533351946\n",
      "F1: 0.24791265253693\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.13574842920640545 \t Test: 0.2866373121002574\n",
      "F1: 0.2482495225970719\n",
      "LR: [0.0005]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 213/213 [00:03<00:00, 53.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([factor \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels)], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     40\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m CrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[0;32m---> 42\u001b[0m fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval_ner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mft_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid2label_conll\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hw_9/model_utils.py:233\u001b[0m, in \u001b[0;36mtrain_eval_ner\u001b[0;34m(model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn, ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size, id2label)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m ft_test_dataloader:\n\u001b[1;32m    232\u001b[0m     predicted_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(ner\u001b[38;5;241m.\u001b[39mpredict(batch)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 233\u001b[0m y_true_ft \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m ft_test_dataset]\n\u001b[1;32m    234\u001b[0m y_pred_ft \u001b[38;5;241m=\u001b[39m predicted_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    235\u001b[0m ft_f1_after \u001b[38;5;241m=\u001b[39m f1_score(y_true\u001b[38;5;241m=\u001b[39my_true_ft, y_pred\u001b[38;5;241m=\u001b[39my_pred_ft)\n",
      "File \u001b[0;32m~/hw_9/model_utils.py:233\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m ft_test_dataloader:\n\u001b[1;32m    232\u001b[0m     predicted_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(ner\u001b[38;5;241m.\u001b[39mpredict(batch)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 233\u001b[0m y_true_ft \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m ft_test_dataset]\n\u001b[1;32m    234\u001b[0m y_pred_ft \u001b[38;5;241m=\u001b[39m predicted_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    235\u001b[0m ft_f1_after \u001b[38;5;241m=\u001b[39m f1_score(y_true\u001b[38;5;241m=\u001b[39my_true_ft, y_pred\u001b[38;5;241m=\u001b[39my_pred_ft)\n",
      "File \u001b[0;32m~/miniconda3/envs/lmenv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2387\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2385\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m   2386\u001b[0m             pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2387\u001b[0m             formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2388\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpa_subtable_ex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2389\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2391\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2392\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_all_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2393\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2394\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lmenv/lib/python3.10/site-packages/datasets/formatting/formatting.py:660\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m python_formatter(pa_table, query_type\u001b[38;5;241m=\u001b[39mquery_type)\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 660\u001b[0m     pa_table_to_format \u001b[38;5;241m=\u001b[39m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m     formatted_output \u001b[38;5;241m=\u001b[39m formatter(pa_table_to_format, query_type\u001b[38;5;241m=\u001b[39mquery_type)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_all_columns:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "mlm_model = BertForMaskedLM.from_pretrained(\"model\")\n",
    "pretrained_bert = mlm_model.bert\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model_ft.bert = pretrained_bert\n",
    "# model_ft.classifier.reset_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for name, param in model_ft.bert.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, model_ft.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=5e-4, betas=(0.9, 0.999), eps=1e-06)\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 16\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"constant\",\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# добавляем веса\n",
    "factor = 1.0\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6fdcfd-842a-4553-9783-522d32d7ea5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "261641f1-e027-4129-a8b0-c4ad2521592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model without fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 31.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 41/41 [00:02<00:00, 19.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Train: 0.8845104447035032 \t Test: 0.8108632375554341\n",
      "F1: 0.10367604824813326\n",
      "LR: [0.0001, 2e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\t Train: 0.6310347246232434 \t Test: 0.4973340506960706\n",
      "F1: 0.13805185704274703\n",
      "LR: [8.888888888888889e-05, 1.7777777777777777e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\t Train: 0.5708514910156481 \t Test: 0.45127149035290975\n",
      "F1: 0.15062923523717328\n",
      "LR: [7.777777777777778e-05, 1.555555555555556e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\t Train: 0.4632657906997984 \t Test: 0.40110956241444845\n",
      "F1: 0.17897977132805631\n",
      "LR: [6.666666666666667e-05, 1.3333333333333333e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t Train: 0.35868929255161885 \t Test: 0.35578864376719405\n",
      "F1: 0.2191635786130228\n",
      "LR: [5.555555555555556e-05, 1.1111111111111113e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\t Train: 0.35356647921639067 \t Test: 0.36214245056233757\n",
      "F1: 0.2314540059347181\n",
      "LR: [4.4444444444444447e-05, 8.888888888888888e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\t Train: 0.34146168055970255 \t Test: 0.3687242425796462\n",
      "F1: 0.22473118279569892\n",
      "LR: [3.3333333333333335e-05, 6.666666666666667e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\t Train: 0.3556981022582851 \t Test: 0.3777718638501516\n",
      "F1: 0.22210414452709887\n",
      "LR: [2.2222222222222223e-05, 4.444444444444444e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\t Train: 0.40477499656518484 \t Test: 0.40453814260843324\n",
      "F1: 0.2043939767958529\n",
      "LR: [1.1111111111111112e-05, 2.222222222222222e-06]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 107/107 [00:06<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t Train: 0.37857362198008 \t Test: 0.3966017576252542\n",
      "F1: 0.21237585943468298\n",
      "LR: [0.0, 0.0]\n",
      "\n",
      "                   pr_base      rec_base       f1_base         pr_ft  \\\n",
      "LOC           0.56 (+0.55)  0.89 (+0.77)  0.69 (+0.68)  0.26 (+0.26)   \n",
      "MISC          0.06 (+0.06)  0.07 (+0.06)  0.06 (+0.05)  0.06 (+0.06)   \n",
      "ORG           0.30 (+0.28)  0.70 (+0.62)  0.42 (+0.39)  0.09 (+0.09)   \n",
      "PER           0.46 (+0.46)  0.79 (+0.77)  0.58 (+0.58)  0.30 (+0.30)   \n",
      "micro avg     0.39 (+0.38)  0.70 (+0.63)  0.50 (+0.49)  0.15 (+0.15)   \n",
      "macro avg     0.34 (+0.33)  0.61 (+0.55)  0.44 (+0.43)  0.18 (+0.18)   \n",
      "weighted avg  0.39 (+0.38)  0.70 (+0.63)  0.50 (+0.49)  0.19 (+0.19)   \n",
      "\n",
      "                    rec_ft         f1_ft  \n",
      "LOC           0.51 (+0.39)  0.34 (+0.34)  \n",
      "MISC          0.21 (+0.21)  0.09 (+0.09)  \n",
      "ORG           0.36 (+0.30)  0.14 (+0.13)  \n",
      "PER           0.47 (+0.46)  0.37 (+0.37)  \n",
      "micro avg     0.39 (+0.35)  0.21 (+0.21)  \n",
      "macro avg     0.39 (+0.34)  0.24 (+0.24)  \n",
      "weighted avg  0.39 (+0.35)  0.25 (+0.25)  \n",
      "\n",
      "metric |           value\n",
      "------------------------\n",
      "f1     |    0.21 (+0.21)\n",
      "pr     |    0.15 (+0.14)\n",
      "rec    |    0.39 (+0.35)\n",
      "acc    |    0.89 (+0.71)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "mlm_model = BertForMaskedLM.from_pretrained(\"model\")\n",
    "pretrained_bert = mlm_model.bert\n",
    "model_ft = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model_ft.bert = pretrained_bert\n",
    "model_ft.classifier.reset_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for p in model_ft.bert.parameters():           \n",
    "    p.requires_grad = False\n",
    "for n, p in model_ft.bert.named_parameters():\n",
    "    if n.startswith((\"encoder.layer.8\",\"encoder.layer.9\",\"encoder.layer.10\", \"encoder.layer.11\")):\n",
    "        p.requires_grad = True\n",
    "for p in model_ft.classifier.parameters():     \n",
    "    p.requires_grad = True\n",
    "\n",
    "head_params = [p for n, p in model_ft.named_parameters() if \"classifier\" in n]\n",
    "bert_params = [p for n, p in model_ft.named_parameters()\n",
    "               if p.requires_grad and \"classifier\" not in n]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": head_params, \"lr\": 1e-4, \"weight_decay\": 0.01},\n",
    "        {\"params\": bert_params, \"lr\": 2e-5, \"weight_decay\": 0.01},\n",
    "    ],\n",
    "    betas=(0.9, 0.999), eps=1e-6\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_factor = 0.1\n",
    "\n",
    "ft_train_dataset = wnut['train']\n",
    "ft_test_dataset = wnut['test']\n",
    "base_test_dataset = conll['test']\n",
    "\n",
    "batch_size = 32\n",
    "batches_per_epoch= len(ft_train_dataset) // batch_size if (len(ft_train_dataset) % batch_size)==0 \\\n",
    "                                                        else len(ft_train_dataset) // batch_size + 1\n",
    "num_training_steps = num_epochs * batches_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_factor * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "factor = 0.01\n",
    "num_labels = 9\n",
    "class_weights = torch.tensor([factor if i == 0 else 1.0 for i in range(num_labels)], device=device)\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "fine_tuned_model, ft_report_before, ft_report_after, ft_metrics_before, ft_metrics_after = train_eval_ner(\n",
    "    model_ft, tokenizer, device, optimizer, num_epochs, lr_scheduler, loss_fn,\n",
    "    ft_train_dataset, ft_test_dataset, base_test_dataset, batch_size,\n",
    "    id2label_conll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9dc2a21b-7994-41b8-b193-cbb80488fd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc566649-fb74-4a3c-a8ec-b25dc1ebe13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5fc1d171-f261-482e-95be-e09ec3ab85b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    {'text' : 'His name is Jerry Abrahamson', 'ner_tags': [0, 0, 0, 1, 2]},\n",
    "    {'text' : 'London is big city', 'ner_tags' : [5, 0, 0, 0]},\n",
    "    {'text' : 'Jason Statham is a famous actor from America', 'ner_tags' : [1, 2, 0, 0, 0, 0, 0, 5]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "98861385-94f6-41cc-a933-a4c97390d5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3627, 1457, 9779, 2312, 1110, 170, 2505, 2811, 1121, 1738, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "test_sentence = test_sentences[idx]['text']\n",
    "\n",
    "tokenizer(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c85eaccd-3ec0-4e93-adb1-a31775241ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b8c3f8b-6c97-4858-a45f-6c1bca37c623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_conll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637c061-1a97-4d5e-a7b0-53706252f020",
   "metadata": {},
   "source": [
    "##### Тест NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "52b87a2c-1875-4f95-a954-4416e0b82122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at model_base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_test = AutoModelForTokenClassification.from_pretrained(\"model_base\")\n",
    "model_test.to(device)\n",
    "tokenizer_test = AutoTokenizer.from_pretrained(\"model_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17b243ee-298d-422e-b2ff-add65d063613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, None]\n",
      "['O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
      "[101, 3627, 1457, 9779, 2312, 1110, 170, 2505, 2811, 1121, 1738, 102]\n",
      "['[CLS]', 'Jason', 'St', '##ath', '##am', 'is', 'a', 'famous', 'actor', 'from', 'America', '[SEP]']\n",
      "Initial model\n",
      "[['O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-PER', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'B-LOC', 'I-LOC']]\n",
      "\n",
      "Fine-tuned model\n",
      "[['O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']]\n",
      "True: O           Pred: O         \n",
      "True: B-PER       Pred: B-PER     \n",
      "True: I-PER       Pred: I-PER     \n",
      "True: I-PER       Pred: I-PER     \n",
      "True: I-PER       Pred: I-PER     \n",
      "True: O           Pred: O         \n",
      "True: O           Pred: O         \n",
      "True: O           Pred: O         \n",
      "True: O           Pred: O         \n",
      "True: O           Pred: O         \n",
      "True: B-LOC       Pred: B-LOC     \n",
      "True: O           Pred: O         \n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import data_prep\n",
    "importlib.reload(data_prep)\n",
    "\n",
    "idx = 2\n",
    "test_sentence = test_sentences[idx]['text']\n",
    "ner_tags = test_sentences[idx]['ner_tags']\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": ner_tags}\n",
    "test_result = data_prep.tokenize_and_preserve_tags(test_example, tokenizer, label2id_conll)\n",
    "print(test_result.word_ids())\n",
    "print(test_result['text_labels'])\n",
    "print(test_result['input_ids'])\n",
    "print([tokenizer.decode(token) for token in test_result['input_ids']])\n",
    "\n",
    "test_batch = {k : [v] for k, v in test_result.items()}\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "batched_test_batch = defaultdict(list)\n",
    "\n",
    "for key, value in test_batch.items():\n",
    "    try:\n",
    "        batched_test_batch[key] = torch.tensor(value)\n",
    "    except ValueError:\n",
    "        batched_test_batch[key] = value\n",
    "\n",
    "for key in batched_test_batch:\n",
    "    if isinstance(batched_test_batch[key], torch.Tensor):\n",
    "        batched_test_batch[key] = batched_test_batch[key].to(device)\n",
    "\n",
    "import importlib\n",
    "import model_utils\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "from model_utils import NamedEntityPredictor\n",
    "\n",
    "print(f\"Initial model\")\n",
    "ner_test = NamedEntityPredictor(model_test, tokenizer, id2label_conll)\n",
    "dt=ner_test.predict(batched_test_batch)\n",
    "print(dt[\"predicted_labels\"])\n",
    "print(f\"\")\n",
    "\n",
    "print(f\"Fine-tuned model\")\n",
    "ner_test = NamedEntityPredictor(fine_tuned_model, tokenizer, id2label_conll)\n",
    "dt=ner_test.predict(batched_test_batch)\n",
    "print(dt[\"predicted_labels\"])\n",
    "\n",
    "# print(f\"Predicted:\\t {ner_test.predict(batched_test_batch)['predicted_labels']}\")\n",
    "# print(f\"True:\\t {batched_test_batch['text_labels']}\")\n",
    "\n",
    "pred = ner_test.predict(batched_test_batch)['predicted_labels']\n",
    "true = batched_test_batch['text_labels']\n",
    "for pred_seq, true_seq in zip(pred, true):\n",
    "    aligned = [(t, p) for t, p in zip(true_seq, pred_seq)]\n",
    "    for t, p in aligned:\n",
    "        print(f\"True: {t:<10}  Pred: {p:<10}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmenv",
   "language": "python",
   "name": "lmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
